# 视觉slam十四讲



## 第 6 讲 非线性优化

**课后习题**：

参考链接：[视觉slam十四讲（第二版）习题-第6章](https://zhuanlan.zhihu.com/p/347762624)

1. **证明线性方程Ax=b 当系数矩阵A超定时，最小二乘解为 $x=(A^TA)^{−1}A^Tb$ ？**

超定方程组是指方程个数大于未知量个数的方程组，即A列满秩且行数大于列数

![img](https://pic2.zhimg.com/80/v2-6a6f19cf9f105d5e2edae80e322679e1_720w.jpg)



2. **调研最速下降法、牛顿法、高斯牛顿法和列文伯格——马夸尔特方法各有什么优缺点？除了Ceres库和g2o库，还有哪些常用的优化库？**

![img](https://pic1.zhimg.com/80/v2-3032bc51403c0affaa9529dbb1906544_720w.jpg)

除了Ceres库和g2o库，还有NLopt库、slam++库等等

3. **为什么高斯牛顿法的增量方程系数矩阵可能不正定？不正定有什么几何含义？为什么在这种情况下解就不稳定了？**

$JJ^T$ 是半正定的，当 J 为零向量时，不正定。此时几何含义是代价函数在此点平坦，导数为0、这种情况下局部的近似为一条水平直线，无法确定参数的更新方向使代价下降。

4. **DogLeg是什么？它与高斯牛顿法和列文伯格——马夸尔特方法有何异同？**

Dogleg属于Trust Region优化方法，即用置信域的方法在最速下降法和高斯牛顿法之间进行切换（将二者的搜索步长及方向转化为向量，两个向量进行叠加得到新的方向和置信域内的步长），相当于是一种加权求解，算法如下：

![img](https://pic3.zhimg.com/80/v2-ab99b56d956baf1082e0d9f540c4b68a_720w.jpg)

高斯牛顿法的解在信赖域中，则取其作为更新量

否则如果最速下降法的解不在信赖域中（即高斯牛顿法和最速下降法的解都不在信赖域中），则将最速下降法的解步长缩短至信赖域半径，作为更新量

否则（如图所示，高斯牛顿法的解不在信赖域中，而最速下降法的解在信赖域中）取两个向量连接起来与信赖域的交点，作为更新量

参考链接：[视觉SLAM十四讲习题答案](https://blog.csdn.net/jiahao62/article/details/80655542?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166234476216782248589451%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166234476216782248589451&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~baidu_landing_v2~default-4-80655542-null-null.nonecase&utm_term=%E8%A7%86%E8%A7%89slam%E5%8D%81%E5%9B%9B%E8%AE%B2%E4%B9%A0%E9%A2%98&spm=1018.2226.3001.4450)

1.系数矩阵超定时，求最小二乘法。

２.参考这两篇文章A,B;常用的优化库：liblbfgs; NLopt...

最速下降法，牛顿法：优：直观方便，求解增量时只需解线性方程即可。

最速下降法：缺：过于贪心，容易走出锯齿路线，反而增加迭代次数。

牛顿法：缺：需要计算目标函数的海塞矩阵，在问题规模较大时非常困难。

高斯牛顿法：优：用J^TJ作为牛顿法H矩阵的近似。

　　　　　　缺：实际中该近似只有半正定性。

列文伯格-马夸尔特法：优：一定程度上避免系数矩阵的非奇异和病态问题。

　　　　　　　　　　缺：收敛速度较慢。

３.Ａand B(为什么不正定，可能为奇异矩阵或病态矩阵），几何含义（MIT线代）马同学二次型这里也有几何介绍　解不稳定　看几何图时只有正定，半正定才有极值点。

补充：１正定矩阵的几何意义　２如何理解二次型

４.DogLeg优化方法:相关材料

5.Ceres-tutorial

6.g2o回来再看

7.代码注释版
