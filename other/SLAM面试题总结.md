# SLAM面试题总结

## 深蓝学院

### 2019年秋招题库参考

> source：[视觉SLAM面试题汇总（2019年秋招题库参考）——第一部分](https://zhuanlan.zhihu.com/p/205008396)

**1 SIFT和SUFT的区别**

构建图像金字塔，SIFT特征利用不同尺寸的图像与高斯差分滤波器卷积；SURF特征利用原图片与不同尺寸的方框滤波器卷积。

特征描述子，SIFT特征有4×4×8=128维描述子，SURF特征有4×4×4=64维描述子

特征点检测方法，SIFT特征先进行非极大抑制，再去除低对比度的点，再通过Hessian矩阵去除边缘响应过大的点；SURF特征先利用Hessian矩阵确定候选点，然后进行非极大抑制

特征点主方向，SIFT特征在正方形区域内统计梯度幅值的直方图，直方图最大值对应主方向，可以有多个主方向；SURF特征在圆形区域内计算各个扇形范围内x、y方向的haar小波响应，模最大的扇形方向作为主方向

**2 相似变换、仿射变换、射影变换的区别**

等距变换：相当于是平移变换（t）和旋转变换（R）的复合，等距变换前后长度，面积，线线之间的角度都不变。自由度为6（3+3）

相似变换：等距变换和均匀缩放（S）的一个复合，类似相似三角形，体积比不变。自由度为7（6+1）

仿射变换：一个平移变换（t）和一个非均匀变换（A）的复合，A是可逆矩阵，并不要求是正交矩阵，仿射变换的不变量是:平行线，平行线的长度的比例，面积的比例。自由度为12（9+3）

射影变换：当图像中的点的齐次坐标的一般非奇异线性变换，射影变换就是把理想点（平行直线在无穷远处相交）变换到图像上，射影变换的不变量是:重合关系、长度的交比。自由度为15（16-1）

参考：多视图几何总结——等距变换、相似变换、仿射变换和射影变换

**3 Homography、Essential和Fundamental Matrix的区别**

Homography Matrix可以将一个二维射影空间的点变换该另一个二维射影空间的点，如下图所示，在不加任何限制的情况下，仅仅考虑二维射影空间中的变换，一个单应矩阵H HH可由9个参数确定，减去scale的一个自由度，自由度为8。



![img](https://pic2.zhimg.com/80/v2-bcb5f7e7e41d3f923d61b8b205aa773d_720w.jpg)

Fundamental Matrix对两幅图像中任何一对对应点x和x′基础矩阵F都满足条件：

![img](https://pic1.zhimg.com/80/v2-8294a3b5ff87f3a1b563246fbcba6630_720w.png)

，秩只有2，因此F的自由度为7。它自由度比本质矩阵多的原因是多了两个内参矩阵。

Essential matrix：本质矩是归一化图像坐标下的基本矩阵的特殊形式，其参数由运动的位姿决定，与相机内参无关，其自由度为6，考虑scale的话自由度为5。

参考多视图几何总结——基础矩阵、本质矩阵和单应矩阵的自由度分析

**4 视差与深度的关系**

在相机完成校正后，则有 d/b=f/z,其中d表示视差，b表示基线，f是焦距，z是深度。这个公式其实很好记，在深度和焦距确定的情况下，基线越大，视差也会越大。



![img](https://pic2.zhimg.com/80/v2-c42354f0ecf11f3cd8f2026fddf9c3f5_720w.jpg)

**5 描述PnP算法**

已知空间点世界坐标系坐标和其像素投影，公式如下

![img](https://pic4.zhimg.com/80/v2-19518bcf9fb5f71d769055d50a5a6d8b_720w.jpg)

目前一共有两种解法，直接线性变换方法（一对点能够构造两个线性约束，因此12个自由度一共需要6对匹配点），另外一种就是非线性优化的方法，假设空间坐标点准确，根据最小重投影误差优化相机位姿。

目前有两个主要场景场景，其一是求解相机相对于某2维图像/3维物体的位姿；其二就是SLAM算法中估计相机位姿时通常需要PnP给出相机初始位姿。

在场景1中，我们通常输入的是物体在世界坐标系下的3D点以及这些3D点在图像上投影的2D点，因此求得的是相机坐标系相对于世界坐标系(Twc)的位姿

在场景2中，通常输入的是上一帧中的3D点（在上一帧的相机坐标系下表示的点）和这些3D点在当前帧中的投影得到的2D点，所以它求得的是当前帧相对于上一帧的位姿变换

**6 闭环检测常用方法**

ORB SLAM中采用的是词袋模型进行闭环检测筛选出候选帧，再通过求解Sim3判断最合适的关键帧

LSD SLAM中的闭环检测主要是根据视差、关键帧连接关系，找出候选帧，然后对每个候选帧和测试的关键帧之间进行双向Sim3跟踪，如果求解出的两个李代数满足马氏距离在一定范围内，则认为是闭环成功

**7给一个二值图，求最大连通域**

这个之后单独写一篇博客来研究这个好了，二值图的连通域应该是用基于图论的深度优先或者广度优先的方法，后来还接触过基于图的分割方法，采用的是并查集的数据结构，之后再作细致对比研究。

**8 梯度下降法、牛顿法、高斯-牛顿法的区别**

在BA优化、PnP、直接法里面都有接触到非线性优化问题，上面几种方法都是针对对非线性优化问题提出的方法，将非线性最优化问题作如下展开，就可以获得梯度下降法和牛顿法

![img](https://pic1.zhimg.com/80/v2-92ccea76d5481ab241e7097bf2860258_720w.png)

梯度下降法是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。因此指保留一阶梯度信息。缺点是过于贪心，容易走出锯齿路线。

![img](https://pic4.zhimg.com/80/v2-237165328c907bf202412156e733cf0f_720w.png)

牛顿法是一个二阶最优化算法，基本思想是利用迭代点处的一阶导数(梯度)和二阶导数(Hessen矩阵)对目标函数进行二次函数近似。因此保留二阶梯度信息。缺点是需要计算H矩阵，计算量太大。

![img](https://pic3.zhimg.com/80/v2-2a43454a053228f6025a15a108a5c8aa_720w.jpg)

而把非线性问题，先进行一阶展开，然后再作平方处理就可以得到高斯-牛顿法和列文博格方法

![img](https://pic4.zhimg.com/80/v2-f4d21334c24bb4aa3a02b98420fd4b0f_720w.jpg)

高斯-牛顿法对上式展开并对Δx进行求导即可得高斯牛顿方程，其实其就是使用

![img](https://pic3.zhimg.com/80/v2-317eb5b46bdfe47b212361321d101506_720w.png)

对牛顿法的H矩阵进行替换，但是

![img](https://pic3.zhimg.com/80/v2-317eb5b46bdfe47b212361321d101506_720w.png)

有可能为奇异矩阵或变态，Δx也会造成结果不稳定，因此稳定性差

![img](https://pic1.zhimg.com/80/v2-aa5449c6487b3ee76d2f68b7cf905e4c_720w.png)

列文博格法就是在高斯-牛顿法的基础上对Δx添加一个信赖区域，保证其只在展开点附近有效，即其优化问题变为带有不等式约束的优化问题，利用Lagrange乘子求解

![img](https://pic3.zhimg.com/80/v2-9586029d1f399e410c18e5df5de35062_720w.jpg)

**9 推导一下卡尔曼滤波、描述下粒子滤波**

用自己的描述下，仅供参考：

卡尔曼滤波：

卡尔曼滤波就是通过运动方程获得均值和方差的预测值，然后结合观测方程和预测的方差求得卡尔曼增益，然后在用卡尔曼增益更行均值和方差的预测值而获得估计值。

卡尔曼滤波推导的思路是（其中一种）先假定有这么一个修正公式

![img](https://pic4.zhimg.com/80/v2-34382b55861aaaf3e7ebdec7dd2235af_720w.png)

构真实值和估计值之间的协方差矩阵，然后通过对对角线元素求和获得方差表达式，我们的修正公式是需要使得方差最小，因此把方差表达式对

![img](https://pic1.zhimg.com/80/v2-28072b4003192af679698ff6eea3f4e8_720w.png)

求导就可以获得卡尔曼增益的表达式，然后从先验到预测值的方差公式可以通过求预测值和真实值的协方差矩阵获得。

粒子滤波：

粒子滤波最常用的是SIR，其算法是用运动方程获得粒子的状态采样，然后用观测方程进行权值更新，通过新的粒子加权平均就获得新的估计状态，最后非常重要的一步就是重采用。

粒子滤波的推导中概念有很多，最重要的推导过程是重要性采样过程，其思路就是我原本的采样分布是不知道的，我如何从一个已知的分布中采样，通过加权的方式使得从已知的分布中采样的粒子分布和原本未知的分布中采样的粒子分布结果一致，从而引入SIS粒子滤波，再进一步加入重采样后就引入了SIR粒子滤波。

具体的可以参看我的另外两个总结博客

概率机器人总结——粒子滤波先实践再推导

概率机器人总结——(扩展)卡尔曼滤波先实践再推导

**10如何求解Ax=b的问题**

参看我的另外一个总结博客多视图几何总结——基础矩阵、本质矩阵和单应矩阵的求解过程

**11 什么是极线约束**

所谓极线约束就是说同一个点在两幅图像上的映射，已知左图映射点p1，那么右图映射点p2一定在相对于p1的极线上，这样可以减少待匹配的点数量。如下图：

![img](https://pic2.zhimg.com/80/v2-bef18d99dbebdc9bdbeeab1512e26a71_720w.jpg)

**12单目视觉SLAM中尺寸漂移是怎么产生的**

用单目估计出来的位移，与真实世界相差一个比例，叫做尺度。这个比例在单目初始化时通过三角化确定，但单纯靠视觉无法确定这个比例到底有多大。由于SLAM过程中噪声的影响，这个比例还不是固定不变的。修正方式是通过回环检测计算Sim3进行修正。

**13解释SLAM中的绑架问题**

绑架问题就是重定位，是指机器人在缺少之前位置信息的情况下，如何去确定当前位姿。例如当机器人被安置在一个已经构建好地图的环境中，但是并不知道它在地图中的相对位置，或者在移动过程中，由于传感器的暂时性功能故障或相机的快速移动，都导致机器人先前的位置信息的丢失，在这种情况下如何重新确定自己的位置。

初始化绑架可以阐述为一种通常状况初始化问题，可使用蒙特卡洛估计器，即粒子滤波方法，重新分散粒子到三维位形空间里面，被里程信息和随机扰动不断更新，初始化粒子聚集到/收敛到可解释观察结果的区域。追踪丢失状态绑架，即在绑架发生之前，系统已经保存当前状态，则可以使用除视觉传感器之外的其他的传感器作为候补测量设备。

**14描述特征点法和直接法的优缺点**

特征点法

优点：1. 没有直接法的强假设，更加精确；2. 相较与直接法，可以在更快的运动下工作，鲁棒性好

缺点：1. 特征提取和特征匹配过程耗时长；2. 特征点少的场景中无法使用；3.只能构建稀疏地图

直接法：

优点：1.省去了特征提取和特征匹配的时间，速度较快；2. 可以用在特征缺失的场合；3. 可以构建半稠密/稠密地图

缺点：1. 易受光照和模糊影响；2.运动必须慢；3.非凸性，易陷入局部极小解

**15EKF和BA的区别**

（1） EKF假设了马尔科夫性，认为k时刻的状态只与k-1时刻有关。BA使用所有的历史数据，做全体的SLAM

（2） EKF做了线性化处理，在工作点处用一阶泰勒展开式近似整个函数，但在工作点较远处不一定成立。BA每迭代一次，状态估计发生改变，我们会重新对新的估计点做泰勒展开，可以把EKF看做只有一次迭代的BA

**16边缘检测算子有哪些？**

边缘检测一般分为三步，分别是滤波、增强、检测。基本原理都是用高斯滤波器进行去噪，之后在用卷积内核寻找像素梯度。常用有三种算法：canny算子，sobel算子，laplacian算子

canny算子：一种完善的边缘检测算法，抗噪能力强，用高斯滤波平滑图像，用一阶偏导的有限差分计算梯度的幅值和方向，对梯度幅值进行非极大值抑制，采用双阈值检测和连接边缘。

sobel算子：一阶导数算子，引入局部平均运算，对噪声具有平滑作用，抗噪声能力强，计算量较大，但定位精度不高，得到的边缘比较粗，适用于精度要求不高的场合。

laplacian算子：二阶微分算子，具有旋转不变性，容易受噪声影响，不能检测边缘的方向，一般不直接用于检测边缘，而是判断明暗变化。

**17 简单实现cv::Mat()**

**18 10个相机同时看到100个路标点，问BA优化的雅克比矩阵多少维**

因为误差对相机姿态的偏导数的维度是2×6,对路标点的偏导数是2×3，又10个相机可以同时看到100个路标点，所以一共有10×100×2行，100×3+10×6个块。

![img](https://pic1.zhimg.com/80/v2-82cbabf256dfbc6b63ac3f4e28a49fc8_720w.png)

**19介绍经典的视觉SLAM框架**

视觉SLAM总结——ORB SLAM2中关键知识点总结

视觉SLAM总结——SVO中关键知识点总结

视觉SLAM总结——LSD SLAM中关键知识点总结

**20介绍下你熟悉的非线性优化库**

非线性优化库一般有ceres和g2o两种，我比较熟悉的是g2o，看下g2o的结构图

![img](https://pic1.zhimg.com/80/v2-c5dc33d30b1f4eb7af4680c02dc075f8_720w.jpg)

它表示了g2o中的类结构。 首先根据前面的代码经验可以发现，我们最终使用的optimizer是一个SparseOptimizer对象，因此我们要维护的就是它(对它进行各种操作)。 一个SparseOptimizer是一个可优化图(OptimizableGraph)，也是一个超图(HyperGraph)。而图中有很多顶点(Vertex)和边(Edge)。顶点继承于BaseVertex，边继承于BaseUnaryEdge、BaseBinaryEdge或BaseMultiEdge。它们都是抽象的基类，实际有用的顶点和边都是它们的派生类。我们用SparseOptimizer.addVertex和SparseOptimizer.addEdge向一个图中添加顶点和边，最后调用SparseOptimizer.optimize完成优化。

在优化之前还需要制定求解器和迭代算法。一个SparseOptimizer拥有一个OptimizationAlgorithm，它继承自Gauss-Newton, Levernberg-Marquardt, Powell’s dogleg三者之一。同时，这个OptimizationAlgorithm拥有一个Solver，它含有两个部分。一个是 SparseBlockMatrix，用于计算稀疏的雅可比和海塞矩阵；一个是线性方程求解器，可从PCG、CSparse、Choldmod三选一，用于求解迭代过程中最关键的一步：

![img](https://pic3.zhimg.com/80/v2-1f8fd9ca2011fafa8879dc494756baea_720w.png)

因此理清了g2o的结构，也就知道了其使用流程。在之前已经说过了，这里就再重复一遍：

（1）选择一个线性方程求解器，PCG、CSparse、Choldmod三选一，来自g2o/solvers文件夹

（2）选择一个BlockSolver，用于求解雅克比和海塞矩阵，来自g2o/core文件夹

（3）选择一个迭代算法，GN、LM、DogLeg三选一，来自g2o/core文件夹

参考G2O图优化基础和SLAM的Bundle Adjustment(光束法平差)

这里我补充下：

注意到上面的结构图中，节点Basevertex<D,T>，BaseBinaryEdge<D,E,VertexXi,VertexXj>和BlockSolver<>等都是模板类，我们可以根据自己的需要初始化不同类型的节点和边以及求解器，以ORB SLAM2为例，分析下后端最典型的全局BA所用的边、节点和求解器：

（1）边是EdgeSE3ProjectXYZ，它其实就是继承自BaseBinaryEdge<2, Vector2d, VertexSBAPointXYZ, VertexSE3Expmap>，其模板类型里第一个参数是观测值维度，这里的观测值是其实就是我们的像素误差u,v u,vu,v，第二个参数就是我们观测值的类型，第三个第四个就是我们边两头节点的类型；

（2）相机节点VertexSE3Expmap，它其实就是继承自BaseVertex<6, SE3Quat>，其模板类第一个参数就是其维度，SE3是六维的这没毛病，第二个就是节点的类型，SE3Quat就是g2o自定义的SE3的类，类里面写了各种SE3的计算法则；

（3）空间点节点VertexSBAPointXYZ，它其实就是继承自BaseVertex<3, Vector3d>，其模板类第一个参数是说明咱空间点的维度是三维，第二个参数说明这个点的类型是Vector3d；

（4）求解器是BlockSolver_6_3，它其实就是BlockSolver< BlockSolverTraits<6, 3> >，6,3分别指代的就是边两边的维度了。

我记得我刚开始学习SLAM的时候自己想办法写后端的时候很纳闷这个图是怎么构建起来的，在ORB或者SVO里面，所有的地图点和关键帧都是以类的形式存在的，例如在ORB中是先将关键帧的节点添加起来，然后添加空间点，然后遍历空间点中记录的与哪些关键帧有关系，然后相应ID的关键帧的节点和空间点的节点连接起来，然后就把图建立起来了，我觉得不写类好像没有什么其他更好的办法了。

**21室内SLAM与自动驾驶SLAM有什么区别？**

这是个开放题，参考无人驾驶技术与SLAM的契合点在哪里，有什么理由能够让SLAM成为无人驾驶的关键技术？

**22 什么是紧耦合、松耦合？优缺点。**

这里默认指的是VIO中的松紧耦合，这里参考深蓝学院的公开课里面介绍：

![img](https://pic2.zhimg.com/80/v2-82b125858ae8035aea08e8623a052031_720w.jpg)

紧耦合是把图像的特征加到特征向量中去，这样做优点是可以免去中间状态的累计误差，提高精度，缺点是系统状态向量的维数会非常高，需要很高的计算量；

松耦合是把VO处理后获得的变换矩阵和IMU进行融合，这样做优点是计算量小但是会带来累计误差。

下面是对经典的VIO框架进行一个分类

![img](https://pic1.zhimg.com/80/v2-bd588deb6aa5b737aa8197241a012c54_720w.jpg)

**23 地图点的构建方法有哪些**

（1）在ORB SLAM2中是根据三角化的方法确定地图点的，利用匹配好的两个点构建AX=b的方程，然后利用SVD分解取最小奇异值对应的特征向量作为地图点坐标，参考多视图几何总结——三角形法

（2）在SVO中是利用深度滤波器进行种子点深度更新，当种子点深度收敛后就加入地图构建地图点。

（在LSD中好像没有维护地图点，不断维护的是关键帧上的深度图）

继续补充…

**24 如果对于一个3D点，我们在连续帧之间形成了2D特征点之间的匹配，但是这个匹配中可能存在错误的匹配。请问你如何去构建3D点？**

毋庸置疑首先想到的是用RANSAC方法进行连续帧之间的位姿估计，然后用内点三角化恢复地图点，具体一点说使用RANSAC估计基础矩阵的算法步骤如下：

（1）从匹配的点对中选择8个点，使用8点法估算出基础矩阵F

（2）计算其余的点对到其对应对极线的距离

![img](https://pic1.zhimg.com/80/v2-d9d171ae81896c9aad44fdb44ff42848_720w.png)

，如果

![img](https://pic2.zhimg.com/80/v2-24f1a964b431cb3f3116711a2b9b194d_720w.png)

则该点为内点，否则为外点。记下符合该条件的内点的个数为

![img](https://pic2.zhimg.com/80/v2-43a9a5d0de011bd5ce6b1004cd0706b9_720w.png)



（4）迭代k次，或者某次得到内点的数目

![img](https://pic2.zhimg.com/80/v2-43a9a5d0de011bd5ce6b1004cd0706b9_720w.png)

占有的比例大于等于95%，则停止。选择

![img](https://pic2.zhimg.com/80/v2-43a9a5d0de011bd5ce6b1004cd0706b9_720w.png)

最大的基础矩阵作为最终的结果。如果是利用非线性优化的方法获得位姿的话，可以在

非线性优化代价函数中加入鲁棒核函数来减少无匹配所带来的误差，例如《视觉SLAM十四讲》里面提到的Huber核

![img](https://pic3.zhimg.com/80/v2-1e8e854ccb9ef4d6fa17b8128e73021a_720w.jpg)

在《机器人的状态估计》一书总将这种方法称为M估计，核函数还包裹Cauchy核

![img](https://pic1.zhimg.com/80/v2-75c27b3c220ac468fbed1316fe6b3b64_720w.jpg)

Geman-MeClure核

![img](https://pic1.zhimg.com/80/v2-6aa4ad22668ad172e462daf8ce29a768_720w.png)

等等。

**25 RANSAC在选择最佳模型的时候用的判断准则是什么?**

简单地说一般是选用具有最小残差和的模型作为最佳模型。



> source：[万字干货！视觉SLAM面试题汇总（19年秋招）——第二部分](https://zhuanlan.zhihu.com/p/212264860)

**视觉SLAM总结——视觉SLAM面试题汇总（后26个）**

**26. 除了RANSAC之外，还有什么鲁棒估计的方法？**

在《机器人的状态估计》一书中还介绍了M估计（广义的最大似然估计）和协方差估计，所谓M估计指的是加入鲁棒代价函数最大似然估计，而**协方差估计**指的是同时估计状态和协方差的方法，也称**自适应估计。**

**27. 3D地图点是怎么存储的？表达方式？**

以ORB SLAM2为例，3D地图点是以类的形式存储的，在类里面除了存储3D地图点的空间点，同时还存储了3D点的描述子（其实就是BRIFE描述子），用来快速进行与特征点的匹配，同时还用一个map存储了与其有观测关系的关键帧以及其在关键帧中的Index等等。

**28. 给你m相机n个点的bundle adjustment。当我们在仿真的时候，在迭代的时候，相机的位姿会很快的接近真值。而地图点却不能很快的收敛这是为什么呢？**

约束相机位姿的方程远多于约束地图点的方程

**29. LM算法里面那个λ是如何变化的呢？**

这里我想从头开始理一遍，参考《视觉SLAM十四讲》首先LM算法优势在哪里，GN法采用雅克比矩阵

![img](https://pic4.zhimg.com/80/v2-b6948450c2510ab67360ae355777f197_720w.png)

的形式来代替难求的海森矩阵，但是

![img](https://pic4.zhimg.com/80/v2-b6948450c2510ab67360ae355777f197_720w.png)

是半正定的，可能出现奇异矩阵或者病态的情况，而且Δx太大的时候也会导致这种二阶泰勒展开的近似不够准确，为解决第二个问题，提出了给Δx添加一个信赖区域方法，也就是LM法，其采用下式判断近似差异的大小进而确定信赖区域范围：

![img](https://pic4.zhimg.com/80/v2-f61bec596738301a098ac3f7c3c2c153_720w.jpg)

其中分析是实际的代价函数下降值，分母是近似下降值。如果ρ越接近1说明近似越准确，ρ过小说明实际下降较小，需要缩小信赖区域范围，如果ρ过大说明实际下降较大，需要扩大信赖区域范围。其步骤如下：

1.初始化

![img](https://pic3.zhimg.com/80/v2-c61a4e24d34a64b8ae1e4225e39ab142_720w.png)

和优化半径μ；

2.进行迭代求解

![img](https://pic2.zhimg.com/80/v2-3fe6a1abbacbf23e1f5e1824e048cbdd_720w.png)

这里D为单位阵是信赖区域范围为一个球形

3.计算ρ

4.如果ρ>3/4，则μ=2μ（扩大信赖区域范围）

5.如果ρ=1/4，则μ=0.5μ（缩小信赖区域范围）

6.如果ρ大于某一阈值，则进行更新

![img](https://pic1.zhimg.com/80/v2-49df150ccc64efebf02ad481b6316ac4_720w.png)

这里面需要优化一个带约束的非线性优化函数，采用拉格朗日乘子法就引入了λ，如下

![img](https://pic2.zhimg.com/80/v2-69a0882486b46819f78e9d8f30a2a3b9_720w.jpg)

求解后获得

![img](https://pic4.zhimg.com/80/v2-7a88d7080250a22c6c9dbe43ce4b3ad3_720w.jpg)

当D=I时有

![img](https://pic4.zhimg.com/80/v2-9c10d5e81ddb5c2d860224fd884d5d4b_720w.png)

求解后当λ较小时说明Δx近似于GN方法求解的结果，二阶是较好的近似，而λ较大时说明近似于一阶梯度下降法，二阶近似效果不够好。

**30. 说一下3D空间的位姿如何去表达?**

李群或者李代数

**31. 李群和李代数的关系**

![img](https://pic3.zhimg.com/80/v2-d8c10d077a0b84bcc3f70bed1d19114e_720w.jpg)

如上图所示（摘自《视觉SLAM十四讲》），从李群到李代数是对数映射，形式上是先取对数，然后取∨，从李代数到李群是**对数映射**，形式上先取∧，再取指数，下面具体说：

**三维旋转：**李群就是三维旋转矩阵，李代数是三维轴角（长度代表旋转大小，方向代表旋转轴方向），从李群到李代数是分别求轴角的角θ(通过矩阵的迹求反余弦)和向量a（旋转矩阵特征值1对应的特征向量），从李代数到李群就是罗德罗杰斯公式。

**三维变换：**李群是四元变换矩阵，李代数是六维向量，从李群到李代数同样先求角和向量，然后需要求t，从李代数到李群的话通过上面的公式计算。

**32. 求导**

![img](https://pic2.zhimg.com/80/v2-2e29d1527f2439378c04dca51366a495_720w.png)

![img](https://pic2.zhimg.com/80/v2-988dfa2119d304859328b7f429b5e879_720w.jpg)

**33. Mat是如何访问元素的？先访问行还是先访问列？**

Mat访问像素一共有三种方法：使用at()方法、使用ptr()方法、使用迭代器、使用data指针

**（1）使用at()方法：**at()方法又是一个模板方法，所以在使用的时候需要传入图像像素的类型，例如：

![img](https://pic2.zhimg.com/80/v2-97f723922793adf21f4f0ac15fbc2171_720w.png)

```cpp
// source:毛星云《OpneCV3编程入门》https://github.com/QianMo/OpenCV3-Intro-Book-Src
void colorReduce11(Mat &image, int div=64) {

	  int nl= image.rows; //行数
	  int nc= image.cols; //列数
              
      for (int j=0; j<nl; j++) 
	  {
          for (int i=0; i<nc; i++) 
		  {
 
            //-------------开始处理每个像素-------------------
                 
                  image.at<Vec3b>(j,i)[0]=	 image.at<Vec3b>(j,i)[0]/div*div + div/2;
                  image.at<Vec3b>(j,i)[1]=	 image.at<Vec3b>(j,i)[1]/div*div + div/2;
                  image.at<Vec3b>(j,i)[2]=	 image.at<Vec3b>(j,i)[2]/div*div + div/2;
 
            //-------------结束像素处理------------------------
 
            } //单行处理结束                 
      }
}
```

**（2）使用ptr()方法：** ptr()方法能够返回指定行的地址（因此正常是先访问行的），然后就可以移动指针访其他的像素。例如

![img](https://pic3.zhimg.com/80/v2-5ebe02cbe02acabe97a3188860ea7b66_720w.png)

这里需要注意的是，有时候在内存中会为了对齐而对末尾的像素有填充，而有时候没有填充。可以使用isContinue()来访问图像是否有填充，对于没有填充的图像，即连续的图像来说，遍历的时候就可以只要一层循环就可以了，他会自己换行将图像变成一维的来处理。

```cpp
void colorReduce0(Mat &image, int div=64) {

    int nl= image.rows; //行数
    int nc= image.cols * image.channels(); //每行元素的总元素数量

    for (int j=0; j<nl; j++) 
    {

        uchar* data= image.ptr<uchar>(j);		// 获取第 j 行的首地址

        for (int i=0; i<nc; i++) 
        {

            //-------------开始处理每个像素-------------------

            data[i]= data[i]/div*div + div/2;		   // 1
            *data++= *data/div*div + div/2;			// 2
            *(data+i)= *data&mask + div/2;		   // 3
	
            //-------------结束像素处理------------------------

        } //单行处理结束                  
    }
}
```



**（3）使用迭代器：**对Mat类型来说，他的迭代器类型可以使用MatIterator\_或者Mat_::Iterator类型，具体使用如下

![img](https://pic3.zhimg.com/80/v2-190758f4d2a0d265e3f140b8fb42e57a_720w.png)

用这两个迭代器便可以指定Mat对象的迭代器，注意需要传入模板参数。对迭代器的初始化与C++中的STL一致。

![img](https://pic2.zhimg.com/80/v2-108e88d2a8af9bb04bfac1cc198bd7cd_720w.png)

遍历也和前面指针一样，从图像左上角第一个像素开始遍历三个字节，然后第二个字节，依次遍历，到第一行遍历完后，就会到第二行来遍历。

```cpp
void colorReduce10(Mat &image, int div=64) {

	  //获取迭代器
	  Mat_<Vec3b> cimage= image;
	  Mat_<Vec3b>::iterator it=cimage.begin();
	  Mat_<Vec3b>::iterator itend=cimage.end();

	  for ( ; it!= itend; it++) { 
        
		//-------------开始处理每个像素-------------------

        (*it)[0]= (*it)[0]/div*div + div/2;
        (*it)[1]= (*it)[1]/div*div + div/2;
        (*it)[2]= (*it)[2]/div*div + div/2;

        //-------------结束像素处理------------------------
	  }
}
```

**（4）使用data指针：**用Mat存储一幅图像时，若图像在内存中是连续存储的（Mat对象的isContinuous == true），则可以将图像的数据看成是一个一维数组，而data（uchar*）成员就是指向图像数据的第一个字节的，因此可以用data指针访问图像的数据，从而加速Mat图像的访问速度。

一般经过裁剪的Mat图像，都不再连续了，如cv::Mat crop_img = src(rect);crop_img 是不连续的Mat图像，如果想转为连续的，最简单的方法，就是将不连续的crop_img 重新clone()一份给新的Mat就是连续的了，例如

![img](https://pic2.zhimg.com/80/v2-d5a4815b8ca418be99cf610bfed33fd9_720w.png)

**34. 写出单目相机的投影模型，畸变模型。**

投影模型一般应该都知道写，但是畸变模型就不一定了…参考《视觉SLAM十四讲》

**投影模型**如下：

![img](https://pic2.zhimg.com/80/v2-bfccd70399680a3e6aa27b8011c38581_720w.jpg)

注意啊，这里空间点是非齐次坐标，而像素变成了齐次坐标，如果空间点也是齐次坐标的话，需要讲变换矩阵写成3×4

的矩阵，最后一列全为0;。

**畸变模型**如下：

畸变模型分为**径向畸变**和**切向畸变**，径向畸变如下：

![img](https://pic1.zhimg.com/80/v2-81b3792d324c18b388aae014d84229c8_720w.jpg)

切向畸变如下：

![img](https://pic4.zhimg.com/80/v2-f1152dbae348bc4d7d794f09ad6cf0df_720w.jpg)

组合上面两式，通过**五个畸变系数**找到空间点在像素平面上的正确位置：

1.将三维空间点P(X,Y,Z)投影到归一化图像平面。设它的归一化坐标为

![img](https://pic1.zhimg.com/80/v2-1a89f14b2bec2f43d3f2781ef97e0858_720w.png)

。

2.对归一化平面上的点进行径向畸变和切向畸变纠正

![img](https://pic2.zhimg.com/80/v2-a57095248974d66509deef53b103608d_720w.png)

3.将纠正后的点通过内参数矩阵投影到像素平面，得到该点在图像上的正确位置

![img](https://pic1.zhimg.com/80/v2-d0a7e2090bb475f3d5e22dd3e6b2b85c_720w.jpg)

值得一提的是，存在两种去畸变处理（Undistort，或称畸变校正）做法。我们可以选择先对整张图像进行去畸变，得到去畸变后的图像，然后讨论此图像上的点的空间位置。或者，我们也可以先考虑图像中的某个点，然后按照去畸变方程，讨论它去畸变后的空间位置。二者都是可行的，不过前者在视觉 SLAM 中似乎更加常见一些。

**35. 安装2D lidar的平台匀速旋转的时候，去激光数据畸变，写代码**

激光雷达里面提到的畸变一般指运动畸变，如果激光数据帧率较同时机器人在运动时就会出现如下图所示情况：

![img](https://pic2.zhimg.com/80/v2-645f5d413435d89f348320a4c631ca79_720w.jpg)

参考激光slam理论与实践（三）：传感器数据处理之激光雷达运动畸变去除

有两种方法：**纯估计方法**和**里程计辅助方法**，其中:

纯估计方法：未知对应点的求解方法，采用极大似然估计方法，而已知对应点的话采用ICP，流程如下：

（1）寻找对应点；

（2）根据对应点，计算R与T；

（3）对点云进行转换，计算误差；

（4）不断迭代，直到误差小于某一值。

里程计辅助方法：用CPU读取激光雷达数据，同时单片机上传里程计数据，两者进行时间同步，在CPU上统一进行运动畸变去除，流程如下：

（1）已知当前激光帧的起始时间

![img](https://pic2.zhimg.com/80/v2-1721fcf227e4c09e0a77efca3b32a071_720w.png)

，

![img](https://pic4.zhimg.com/80/v2-88314de48b64fd21f99c0dd82a39e4af_720w.png)

（2）两个激光束间的时间间隔∗t

（3）里程计数据按照时间顺序存储在一个队列里。

（4）求解当前帧激光数据中的每一个激光点对应的里程计数据（即机器人位姿）

（5）根据求解的位姿把所有的激光点转换到同一坐标系下

（6）重新封装成一帧激光数据发布出去

**36. 给两组已经匹配好的3D点，计算相对位姿变换，写代码**

匹配两组已知坐标的3D点当然是采用ICP，参考《视觉SLAM十四讲》，ICP的解法一共有两种：**SVD方法**和**非线性优化方法**，下面过一遍SVD方法的推导过程：

![img](https://pic1.zhimg.com/80/v2-b862e115cc1344b4be42f2ffccf46f28_720w.jpg)

构建最小二乘的代价函数，求得使误差平方和达到最小的R,t

![img](https://pic1.zhimg.com/80/v2-123585b88e51f61965a31e4fade712f8_720w.jpg)

定义两组点的质心

![img](https://pic1.zhimg.com/80/v2-d62958651478a7f599b35517248cb1f0_720w.jpg)

对代价函数做如下处理：

![img](https://pic1.zhimg.com/80/v2-4305e812c3e8013ebf71a03fa973de08_720w.jpg)

上面三项中最后一项求和为零，因此代价函数变为

![img](https://pic2.zhimg.com/80/v2-d44e792aa3fffe39a8b8902a1069e941_720w.png)

第一项只和R有关，因此我们可以先求得一个R使得第一项最小然后再求t，我们记去质心的点分别为

![img](https://pic3.zhimg.com/80/v2-8f57f4532d5164f04320e009550dcd22_720w.png)

和

![img](https://pic4.zhimg.com/80/v2-c67e43984751cdb7b8f318470171af07_720w.png)

，我们对第一项展开得：

![img](https://pic3.zhimg.com/80/v2-240c760d5cd7f9b6f69ec84233634d0a_720w.png)

第一项和第二项都与R无关，因此最后优化目标函数变为：

![img](https://pic4.zhimg.com/80/v2-f7ed8736ee1fda753c627154914e8227_720w.png)

最后通过SVD方法求得使得上述代价函数最小的R，先定义矩阵：

![img](https://pic3.zhimg.com/80/v2-55333579050ad16e39a53b9038d5a38a_720w.png)

对其进行SVDF分解

![img](https://pic3.zhimg.com/80/v2-fd42b829213bacb4a7fb81f1c57e42be_720w.png)

当W满秩时，R为：

![img](https://pic2.zhimg.com/80/v2-7c4987090cd203e760628e82ba6007e5_720w.png)

解得R后就可以进一步求得t。代码如下：

![img](https://pic3.zhimg.com/80/v2-02967e58fd210849c259e75157ec1f4a_720w.jpg)

![img](https://pic4.zhimg.com/80/v2-1178d99470cc122bb712fc9b3577b537_720w.jpg)

**37. ORB-SLAM初始化的时候为什么要同时计算H矩阵和F矩阵？**

简单地说，因为初始化的时候如果出现纯旋转或者所有特征点在同一个平面上的情况，F矩阵会发生自由度退化，而这个时候H矩阵会有较小误差，因此要同时计算H矩阵和F矩阵，那么这里补充两个问题：

**（1）ORB SLAM是怎样选用哪个矩阵去恢复旋转和平移的呢？**

这部分代码是这个样子的：

![img](https://pic3.zhimg.com/80/v2-bc7b670a7ea9af5c8854797094b87052_720w.jpg)

计算SF和SH的公式如下：

![img](https://pic1.zhimg.com/80/v2-7d0a97d059286c27e069a7184615992c_720w.png)

其中：

![img](https://pic2.zhimg.com/80/v2-5de795aba5873a116e0a52f1774e1ab5_720w.jpg)

然后SH和SF的比值公式如果结果大于0.4的话就选择H矩阵，如果小于0.4的话就选择F矩阵来进行初始化。

**（2）F矩阵退化会发生在哪些情况下？**

F矩阵会在两种条件下发生退化，准确地说是三种，第一种是发生在仅旋转的情况下，第二种是发生在所有空间点共面的情况下，第三种是所有空间点和两个摄像机中心在一个二次曲面上，有可能发生退化（第三种情况暂时不予讨论，可参看《多视图几何》一书），下面我们来看下他们为什么会退化：

**第一种情况：**仅发生旋转，这个比较好理解，基础矩阵满足

![img](https://pic3.zhimg.com/80/v2-3b54c4ba68f23668817129cbe0ebc6fe_720w.png)

在这种情况下，t是零向量，此时求得的基础矩阵是零矩阵，因此无法通过下面的公式求得基础矩阵

![img](https://pic2.zhimg.com/80/v2-4d31f063ddec2c8054edb121b5763a9d_720w.png)

**第二种情况：**所有空间点在一个平面上，这种情况下，匹配点的点集

![img](https://pic1.zhimg.com/80/v2-d277d521d4f3304dd97f01c2b312c68c_720w.png)

满足射影棉变换，即

![img](https://pic3.zhimg.com/80/v2-9e319708608b8d81c92887c87596b3c6_720w.png)

，这时基础矩阵的方程变为

![img](https://pic3.zhimg.com/80/v2-19f8496cf03a0e51387b81f7ba04c4d2_720w.png)

注意这时只要

![img](https://pic4.zhimg.com/80/v2-ea6593069faad386fbb54c6f3358564f_720w.png)

是一个任意的反对称矩阵都满足这个方程，因此F矩阵可以写成

![img](https://pic3.zhimg.com/80/v2-8c712aac3ebeb2e2d76ca8b886289f32_720w.png)

S为任意的反对称矩阵，因此这种情况下只能求出来的F矩阵是一个三参数簇，而不是一个具体的解。

这里再补充一点，我们还要区分好退化和简化的区别，什么情况下会发生F矩阵的简化呢？

**第一种情况：**纯平移运动（就是沿着相机坐标系的z轴运动），这种情况下F矩阵简化成了一个反对称矩阵，并且只有两个自由度（反对称矩阵并且尺度不变性），因此两组匹配点就可以求解这种情况，因此这种情况下，上面**退化的第二种情况就不会发生了，**因为两组匹配点构成的两个空间点肯定都是公面的。

**第二种情况：**纯平面运动（就是沿着相机坐标系的x轴运动），这种情况下F矩阵的对称部分秩为2（具体为什么可能需要查资料推导了），所以会在原本的F矩阵上再添加一个约束，使得自由度变成六个自由度。

第三种情况：标定之后的情形，其实就是F矩阵在把内参获得之后就变成了E矩阵,自由度变成五个自由度，这个没什么好说的。

**38. 说一下Dog-Leg算法**

参考非线性最小二乘法之Gauss Newton、L-M、Dog-Leg

Dog-Leg算法是一种高斯牛顿法和最速下降法混合使用的方法，LM法也是这样一种方法，这两者不同的是，LM法采用的是使用一个阻尼项λ来控制下降的速度在一个合理的半径内，如果λ较小的话说明二阶近似较好，方法更加接近于高斯牛顿法，如果λ较大的话说明二阶近似较差，方法更加接近毓最速下降法

Dog-Leg算法是怎么做的呢？在这之前我们要先回顾下最速下降法和高斯牛顿法中：

**最速下降法：**在《视觉SLAM十四讲》中也提到，最速下降法的增量方向是

![img](https://pic4.zhimg.com/80/v2-30e3f8c01d75df39f447122d989cc29f_720w.png)

![img](https://pic4.zhimg.com/80/v2-3fdd883c58c55c7cf324564b2586eb37_720w.png)

，沿着反向梯度方向前进一个补偿λ即可进行梯度下降，那么λ取多少合适呢？十四讲中并没有说，Dog-Leg算法中给出了评判标准：

假设

![img](https://pic4.zhimg.com/80/v2-b9020dd84d42f6286c1a3d903edcceeb_720w.png)

：

![img](https://pic1.zhimg.com/80/v2-a6a32a8c03a8ac54cfc38e66bacdb830_720w.jpg)

其中

![img](https://pic1.zhimg.com/80/v2-1fcede6509f3f56a5cd85671939a5e64_720w.png)

是最速下降法下降方向，使得上式最小，对α 求导得

![img](https://pic1.zhimg.com/80/v2-3380016110d8b7d49b04fe4942ba354c_720w.jpg)

因此对于最速下降法有

![img](https://pic3.zhimg.com/80/v2-f915434d428707b358b67abe410ddad6_720w.png)

**高斯牛顿法：**这种方法当中是可以同时求得下降方向和下降大小的

![img](https://pic4.zhimg.com/80/v2-121928a0fe514d7552a051c2e02345c3_720w.png)

然后接着介绍**信赖域**，所谓信赖域就是将下降范围控制在这个区域内，在这个范围内二阶泰勒展开能有较好的近似，也即是说不管我们是选择高斯牛顿法还是最速下降法都需要满足

![img](https://pic2.zhimg.com/80/v2-f270dbded0872b49f3bd60d8a200e8ed_720w.png)

,二阶近似才能较好成立，因此Dog-Leg法给出了如下准则：

![img](https://pic2.zhimg.com/80/v2-91f892f10fb629a22d5687a8e1012dc9_720w.jpg)

其中

![img](https://pic2.zhimg.com/80/v2-b250309d8d00cadb34063d0adbd83459_720w.png)

为，上式中第一种情况迭代后下降的点为B点（因为是从另一个博客扒的图，所以里面符号不一样，其中pB指的是高斯牛顿的下降方向，pU指的是最速下降法下降方向）

![img](https://pic3.zhimg.com/80/v2-0846fa48fec98e6d94978898a8244bd6_720w.jpg)

**第二种情况**为迭代后下降的点为**黄色星星点**

![img](https://pic4.zhimg.com/80/v2-aecbab58e35c2eb9f9e9998703488e2f_720w.jpg)

**第二种情况**为迭代后下降的点为**黄色星星点**

![img](https://pic3.zhimg.com/80/v2-85d4e9ff2a057364b1c19503aa838a3e_720w.jpg)

由此可见通过上式成功地将下降区域控制在了信赖区域内，那么信赖区域的半径Δ是怎么更新的呢？如下：

![img](https://pic1.zhimg.com/80/v2-013eeac822bacbd186a6aecbdbdad984_720w.jpg)

其中

![img](https://pic1.zhimg.com/80/v2-df1ecb7ab97b28cef90b595a7de934c0_720w.jpg)

综上所述，Dog-Leg的步骤如下：

step1：初始化

![img](https://pic4.zhimg.com/80/v2-5e2562056e01efa88649a35d792a26c3_720w.png)

step2：求解梯度

![img](https://pic1.zhimg.com/80/v2-d8b30d5ab0e6663a1bc49bd120284828_720w.png)

，如果

![img](https://pic1.zhimg.com/80/v2-50893fa173974fb8ffad1db02144b008_720w.png)

，则退出，否则继续。如果

![img](https://pic1.zhimg.com/80/v2-914b2b5cbdb342c8b2d66cb3a0dee3ac_720w.png)

，则退出，否则继续。

step3：如果半径

![img](https://pic3.zhimg.com/80/v2-016abecf628c7985597b5b226342c322_720w.png)

，则退出迭代；否则继续；

step4：分别根据GaussNewton法和最快下降法计算

![img](https://pic3.zhimg.com/80/v2-e093ea1a9be34b04832c5fd23de7c89e_720w.png)

和

![img](https://pic4.zhimg.com/80/v2-38c00a10bbab65c857cb148e34559787_720w.png)

，然后计算最快下降法的迭代步长

![img](https://pic2.zhimg.com/80/v2-1cd53a2ea1a99e10e6fbc4520ae0a53d_720w.png)

。

step5：根据

![img](https://pic3.zhimg.com/80/v2-e093ea1a9be34b04832c5fd23de7c89e_720w.png)

，

![img](https://pic4.zhimg.com/80/v2-38c00a10bbab65c857cb148e34559787_720w.png)

和信赖区域半径

![img](https://pic3.zhimg.com/80/v2-de63b835e714532001cbd55d8dc55aca_720w.png)

，来计算Dog-Leg步进值

![img](https://pic3.zhimg.com/80/v2-063ff9f130bd83ab73779927d21ded1a_720w.png)

。若

![img](https://pic2.zhimg.com/80/v2-1b6f58017b4124fbf5a000860bd20159_720w.png)

![img](https://pic3.zhimg.com/80/v2-eaf6edc72e6f74cdf34b108e26d7e39e_720w.png)

，则退出迭代；否则继续。

step6：

![img](https://pic3.zhimg.com/80/v2-48bc294dedd391e8f03b823948355afa_720w.png)

，计算增益比

![img](https://pic2.zhimg.com/80/v2-5ec81849f3631d188358c5bdcb626e65_720w.png)

![img](https://pic1.zhimg.com/80/v2-5c731eda90539783727fa88da57bf60c_720w.jpg)

重复step2。

对于ϵ1,ϵ2,ϵ3可以选取任意小的值如

![img](https://pic2.zhimg.com/80/v2-c4c985590c4525b23a9790341263ade9_720w.png)

，只是作为迭代的终止条件，其值得选取对最终的收敛结果影响不大。

对比可以进一步发现LM法是通过阻尼器λ控制下降范围的，λ的不同会导致LM法跟接近于高斯牛顿法还是更接近于最速下降法，而Dog-Leg是先计算高斯牛顿法和最速下降法的结果，然后根据两者结果以及信赖区域半径来确定最后迭代采用那个结果。

**39. Vins-Mono里面什么是边缘化？First Estimate Jacobian？一致性？可观性？**

边缘化其实简单说就是将滑窗中丢弃的图像帧的信息保留下来传递给剩余变量的方式

First Estimate Jacobian是为了解决新测量信息和旧的先验信息构建新的系统时，对某一优化变量求雅克比的线性化点不同导致信息矩阵的零空间发生变化，不可观的变量变成可观变量的问题，做法就是保证变脸的线性化点不变。

一致性应该指的就是线性化点的一致不变，而可观性的定义和现代控制理论中能观性定义是一致的，即通过测量获得状态变量的信息，即该变量是能观的这里给出在深蓝学院的课程中给定一种定义：

对于测量系统 z=h(θ)+ε, 其中

![img](https://pic4.zhimg.com/80/v2-1fb48254d174e60c7ebc293110a25ac3_720w.png)

为测量值， θ∈Rd为系统状态量,ε为测量噪声向量。h(·)是个非线性函数，将状态量映射成测量。对于理想数据，如果以下条件成立，则系统状态量θ可观：

![img](https://pic4.zhimg.com/80/v2-39be636f1e08d727bcb1001625f3144f_720w.png)

**40. 说一下VINS-Mono的优缺点**

VINS-Mono缺点网上总结得好像不是很多，我根据我的经验总结下面几个缺点：

（1）VINS-Mono的前段是采用的提取关键点然后采用光流法追踪，因此对于弱纹理，关键点少的环境鲁棒性和精度差；

（2）同样还是因为前段的问题，因为没有提取特征描述子，而是使用光流法进行的追踪匹配，一旦画面模糊或者图像丢失，相机就会丢，而且没有重定位模块；

（3）在恒速运动下，会使得IMU有一个自由度不客观，因此会发生漂移。

**41. 推导一下VINS-Mono里面的预积分公式**

参考博客VINS-Mono关键知识点总结——预积分和后端优化IMU部分

**42. 在给定一些有噪声的GPS信号的时候如何去精准的定位？**

**43. 如何标定IMU与相机之间的外参数？**

目前我还没有实际标定过，标定方法可以参考贺博的博客Kalibr 标定双目内外参数以及 IMU 外参数，像Intel出的D435i是已经标定号外参数的，另外在VINS-mono中可以对相机的外参数进行估计。

**44. 给你xx误差的GPS，给你xx误差的惯导你怎么得到一个cm级别的地图?**

**45. 计算H矩阵和F矩阵的时候有什么技巧呢？**

其中我能想到的技巧有两点，第一个是RANSAC操作，第二个是归一化操作，RANSAC操作前面已经解释过了，这里主要来分析下归一化操作，在《多视图几何》中提到了一种归一化八点法，方法是先用归一化矩阵对图像坐标进行平移和尺度缩放，然后利用八点法求解单应或者基础矩阵，最后再利用归一化矩阵恢复真实的单应或者基础矩阵，归一化具体操作和优势如下：

**具体操作：**又称各项同性缩放（非同性缩放有额外开销，但是效果并未提升），步骤如下

（1）对每幅图像中的坐标进行平移（每幅图像的平移不同）使点集的形心移至原点

（2）对坐标系进行缩放使得点x=(x,y,w)中的x,y,w总体上有一样的平均值，注意，对坐标方向，选择的是各向同性，也就是说一个点的x和y坐标等量缩放

（3）选择缩放因子使得点x到原点的平均距离等于

![img](https://pic2.zhimg.com/80/v2-ef194261ed9bb9eb714b3f46723b6d75_720w.png)

**优势：**

（1）提高了结果的精度；

（2）归一化步骤通过为测量数据选择有效的标准坐标系，预先消除了坐标变换的影响，使得八点法对于相似变换不变。

**46. 给一组点云，从中提取平面。**

应该有很多方法的，慢慢补充：

**（1）区域生长法：**首先依据点的曲率值对点进行排序，之所以排序是因为，区域生长算法是从曲率最小的点开始生长的，这个点就是初始种子点，初始种子点所在的区域即为最平滑的区域，从最平滑的区域开始生长可减少分割片段的总数，提高效率，设置一空的种子点序列和空的聚类区域，选好初始种子后，将其加入到种子点序列中，并搜索邻域点，对每一个邻域点，比较邻域点的法线与当前种子点的法线之间的夹角，小于平滑阀值的将当前点加入到当前区域，然后检测每一个邻域点的曲率值，小于曲率阀值的加入到种子点序列中，删除当前的种子点，循环执行以上步骤，直到种子序列为空

**（2）随机抽样一致算法**

**（3）基于凸包的凹点挖掘算法：**

1.提取点云的凸包

2.计算凸包每条边的顶点的点密度（即该点 K 个临近点到该点的距离平均值）

3.如果顶点点密度大于所在边的长度的 X 倍，则删除该边，并从内部点中选择出一个满足夹角最大的点，插入边界边，形成两条新的边界边

4.迭代 2 和 3，一直到全部边界边的 X 倍小于其端点的点密度，算法结束

**（4）基于 Delaunay 三角网的轮廓提取算法：**

**A**. 不使用辅助点：

1.首先对点云进行 Delaunay 三角构网

2.同上，判断每条网格边长度的X倍和其端点的点密度之间的大小关系，并删除长的网格边

3.提取只属于一个三角形的边界，作为边界边

4.分类排序，得到有顺序关系的内外轮廓

**B**. 使用辅助点：

1.手动在点云的边界附近选点

2.Delaunay构网

3.判断每个三角形，如果其中一个点是辅助点，而另外两个点是点云中的点，则连接这两个点做为边界边

4.分类排序，得到有顺序关系的内外轮廓

参考提取平面点云的轮廓

**47. 给一张图片，知道相机与地面之间的相对关系，计算出图的俯视图。**

参考如何计算一张图片的俯视图？

简单地说利用射影变换，将原本不垂直的线垂直化（用多视图几何上的话说就是消除透视失真），如下图所示



![img](https://pic2.zhimg.com/80/v2-058993c0de35e017dc0adac9d4f00105_720w.jpg)

**理论推导**如下：

从世界坐标系到图像坐标系的变换如下：

![img](https://pic1.zhimg.com/80/v2-cf8bd0bd22189a9936d3bc5acecabed0_720w.jpg)

上面的透视变换（射影变换）是将一个平面上的点投影到另外一个平面上去，因此上面的空间点[x0,y0,z0,1] 也在同一平面上，我们不妨设第三维坐标为0，有：

![img](https://pic1.zhimg.com/80/v2-4a7083a94637bb05f2e63f9fa506da54_720w.jpg)

上式可以简化为

![img](https://pic4.zhimg.com/80/v2-798e12de5c6c7eee8200d8e8750bebbb_720w.jpg)

这就变化了求解一个单应矩阵，采用四对点就可以进行求解。

因此针对上面那个例子我们的实际操作步骤如下：

（1）灰度化处理

（2）滤波处理

（3）边缘检测

（4）寻找四个点——霍夫变换直线识别

（5）计算 H 矩阵

（6）消除透视失真

**48. 双线性插值如何去做，写公式。**

有同学肯定会好奇为嘛会有这个题，这个问题是承接上一个问题来的，在进行透视变换时会遇到的一个实际问题如下图所示

![img](https://pic2.zhimg.com/80/v2-309e1eff1f780243bdb2978ea027aa49_720w.jpg)

右图（原始图像）中的p点像素(x0,y0)为整数，而到左图中)（变换后的图像）中的p′点像素(x′0,y′0)就不一定是整数，这如何操作呢？一般就是用双线性插值去做。

我们可以发线，p′会落在(x1,y1),(x1+1,y1),(x1+1,y1+1),(x1,y1+1)这四个相邻点的中间，因此我们就要利用(x1,y1),(x1+1,y1),(x1+1,y1+1),(x1,y1+1)，(x1,y1+1),的像素值来计算(x′0,y′0)这点的像素值

![img](https://pic4.zhimg.com/80/v2-8af3d94cf6e5c019c6f536e69cd21b43_720w.jpg)

其实很好记忆的，看下面这张图

![img](https://pic3.zhimg.com/80/v2-76064964997b5cf17083c7ac43848f72_720w.jpg)

写公式的话记住(x1+1,y1+1)前面的系数是ab abab

**49. RGB-D的SLAM和RGB的SLAM有什么区别？**

网上讨论这个的实在太多，我个人觉得单目比较困难点的就是初始化（纯旋转不行，对着平面不行）和尺度问题（需要用Sim解决回环），RGBD-SLAM的话因为有深度因此尺度问题解决了，再环境重建方面会有天然的优势…答得不全，可以再作补充

**50. 什么是ORB特征? ORB特征的旋转不变性是如何做的? BRIEF算子是怎么提取的?**

ORB特征指的是Oriented FAST and rotated BREIF，包括改进后的FAST角点和BREIF特征子，ORB特征的旋转不变形主要是通过计算半径r范围内像素点的一阶矩，连接质心到特征点的向量作为主方向来对周围像素进行旋转，然后提取BRIEF特征子，BRIEF特征描述子通过计算出来的一个二进制串特征描述符来进行提取的。

**51. ORB-SLAM中的特征是如何提取的？如何均匀化的？**

**ORB描述子的提取流程：**

**1.输入图像，并对输入图像进行预处理，将其转换成灰度图像；**

**2.初始化参数，**包括特征点数量nfeatures，尺度scaleFactor，金字塔层数nlevel，初始阈值iniThFAST，最小阈值minThFAST等参数；

**3.计算金字塔图像，**使用8层金字塔，尺度因子为1.2，则通过对原图像进行不同层次的resize，可以获得8层金字塔的图像；

**4.计算特征点：**

（1）将图像分割成网格，每个网格大小为WW=3030像素；

（2）遍历每个网格；

（3）对每个网格提取FAST关键点，先用初始阈值iniThFAST提取，若提取不到关键点，则改用最小阈值minThFAST提取。（注意，初始阈值一般比最小阈值大）

**5.对所有提取到的关键点利用八叉树的形式进行划分：**

（1）按照像素宽和像素高的比值作为初始的节点数量，并将关键点坐标落在对应节点内的关键点分配入节点中；

（2）根据每个节点中存在的特征点数量作为判断依据，如果当前节点只有1个关键点，则停止分割。否则继续等分成4份；

（3）按照上述方法不断划分下去，如图所示，可见出现一个八叉树的结构，终止条件是节点的数目Lnode大于等于要求的特征点数量nfeatures；

（4）对满足条件的节点进行遍历，在每个节点中保存响应值最大的关键点，保证特征点的高性能；

![img](https://pic4.zhimg.com/80/v2-6b5a0548447d7fbb91c2c1c3f015e713_720w.jpg)

**6.对上述所保存的所有节点中的特征点计算主方向，**利用灰度质心的方法计算主方向，上一讲中我们已经讲解过方法，这讲就不再赘述了；

**7.对图像中每个关键点计算其描述子，**值得注意的是，为了将主方向融入BRIEF中，在计算描述子时，ORB将pattern进行旋转，使得其具备旋转不变性；参考ORBSLAM2中ORB特征提取的特点



> source：[2022最新SLAM面试题汇总（持续更新中）](https://zhuanlan.zhihu.com/p/544043873)

目前机器人SLAM问题是一个非常值得研究的方向，在未知环境中，首先要通过SLAM技术获得环境的地图，然后才能进行导航。这个方向是近几年比较新的研究方向，相关的机器人公司以及研究机器人的大厂也很需要SLAM方向的人才，比如大疆、美团、旷视科技等已经在这个行业有了一定的产品应用。在SLAM方向的面试中，总结的面试题如下：

**1.重定位和回环检测的区别是什么？**

重定位是跟丢以后重新找回当前的姿态，通过当前帧和关键帧之间的特征匹配，定位当前帧的相机位姿。重定位就是重新定位，当前图像因为和最近的图像或者局部地图之间缺乏足够的匹配，导致机器人无法确定自己的位姿，此时处于当前状态的机器人不再知道其在地图中的位置，也叫做机器人被“绑架”，就说的是人质被蒙上双眼带到未知地方，蒙罩去掉后完全不知道自己在哪里，这时候就需要充分利用之前建好的地图或者存好的数据库。此时机器人需要观察周围环境，并且从已有地图中寻找可靠的匹配关系，一般是关键帧信息，这样就可以根据已有信息“重新”估计机器人的姿态。

回环检测是为了解决位置估计随时间漂移的问题。主要是通过识别曾经到过的场景，将其与当前帧对应，优化整个地图信息，包括3D路标点、相机位姿和相对尺度信息。回环的主要目的是降低机器人随时间增加，轨迹中累积的漂移，一般发生在建图过程中。这是因为基于运动传感器或者视觉信息的里程计容易出错，使估计的轨迹偏离其实际真实的情况。通过回环，优化整个地图信息，包括3D路标点、相机位姿和相对尺度信息。回环检测提供了回环帧与所有历史帧的关系，可以极大减少误差。回环主要是纠正机器人/相机轨迹，而重新定位再从未知状态找回姿态。两者都需要当前图像预先访问过之前的位置附近，本质上都是一个图像识别问题。

重定位和回环检测的区别：

重定位主要为了恢复姿态估计，而回环是为了解决漂移，提高全局精度。二者容易混淆的原因是重定位通常也需要找到与之前帧的对应关系求解出姿态，而这可以通过回环来完成，很多算法是可以共享的。

**2.单应矩阵H和基础矩阵F的区别是什么？**

（1）基础矩阵F和单应矩阵H所求相机获取图像状态不同而选择不同的矩阵。

（2）本质矩阵E和基础矩阵F之间相差相机内参K的运算。

（3）只旋转不平移求出F并分解出的R，T和真实差距大不准确，能求H并分解得R。

**3.视觉SLAM方法的分类和对应的特点分析。**

视觉SLAM可以分为特征点法和直接法。特征点法是根据提取、匹配特征点来估计相机运动，优化的是重投影误差，对光照变化不敏感，是比较成熟的方案，常见的开源方法有ORB-SLAM等。

特征点法的优点：

（1）特征点本身对光照、运动、旋转比较不敏感，因此稳定性更好。

（2）相机运动较快，也能跟踪成功，鲁棒性较好。

（3）研究时间较久，方案比较成熟。

特征点法的缺点：

（1）关键点提取、描述子匹配时间长。

（2）特征点丢失的场景无法使用。

（3）只能构建稀疏地图。

直接法根据相机的亮度信息估计相机的运动，可以不需要计算关键点和描述子，优化的是光度误差，根据使用像素可分为稀疏、半稠密、稠密三种，常见的方案是SVO、LSD-SLAM等。

直接法的优点：

（1）速度快，可以省去计算特征点和描述子时间。

（2）可以在特征缺失的场合，特征点法在该情况下会急速变差。

（3）可以构建半稠密乃至稠密地图。

直接法的缺点：

（1）因为假设了灰度不变，所以易受光照变化影响。

（2）要求相机运动较慢或采样频率较高。

（3）单个像素或像素块区分度不强，采用的是数量代替质量的策略。

**4.关键帧的作用是什么？**

关键帧目前是一种非常常用的方法，可以减少待优化的帧数，并且可以代表其附近的帧。

**5. 如何选择关键帧？**

选取关键帧的指标：

（1）距离上一关键帧的帧数是否足够多（时间）。运动很慢的时候，就会选择大量相似的关键帧，冗余、运动快的时候又丢失了很多重要的帧。

（2）距离最近关键帧的距离是否足够远（空间）运动。相邻帧根据姿态计算运动的相对大小，可以是位移，也可以是旋转，或者二者都考虑了。

（3）跟踪质量（主要根据跟踪过程中搜索到的点数和搜索的点数比例）/共视特征点。这种方法记录了当前视角下的特征点数或者视角，当相机离开当前场景时才会新建关键帧，避免了上一种方法存在的问题，缺点是比较复杂。

**6.相机传感器的分类及其优缺点是什么？**

视觉SLAM常用的相机包括单目相机、双目相机和深度相机。

单目相机的优点：

（1）应用最广，成本可以做到非常低。

（2）体积小，标定简单，硬件搭建也简单。

（3）在有适合光照的情况下，可以适用于室内和室外环境。

单目相机的缺点：

（1）具有纯视觉传感器的通病：在光照变化大，纹理特征缺失、快速运动导致模糊的情况下无法使用。

（2）SLAM过程中使用单目相机具有尺度不确定性，需要专门的初始化。

（3）必须通过运动才能估计深度，帧间匹配三角化。

双目相机一般有Indemind、小觅和ZED等。

双目相机的优点：

（1）相比于单目相机，在静止时就可以根据左右相机视差图计算深度。

（2）测量距离可以根据基线调节。基线距离越大，测量距离越远。

（3）在有适合光照的情况下，可以适用于室内和室外。

双目相机的缺点：

（1）双目相机标定相对复杂。

（2）用视差计算深度比较消耗资源。

（3）具有纯视觉传感器的通病：在光照变化较大、纹理特征缺失、快速运动导致模糊的情况下无法使用。

深度相机一般有Kinect系列、Realsense系列、Orbbec和Pico等。

深度相机的优点：

（1）使用物理测距方法测量深度，避免了纯视觉方法的通病，适用于没有光照和快速运动的情况。

（2）相对双目相机，输出帧率较高，更适合运动场景。

（3）输出深度值比较准，结合RGB信息，容易实现手势识别、人体姿态估计等应用。

深度相机的缺点：

（1）测量范围窄，容易受光照影响，通常只能用于室内场景。

（2）在遇到投射材料、反光表面、黑色物体情况下表现不好，造成深度图确实。

（3）通常分辨率无法做到很高，目前主流的分辨率是640×480.

（4）标定比较复杂。

**7.ROS中rosrun和roslaunch的区别是什么？**

1. rosrun允许在任意软件包中运行可执行文件，而无需先在其中进行cd或roscd。

2. roslaunch可以通过ssh在本地和远程轻松启动多个ros节点，以及在参数服务器上设置参数。它包括自动重生已经死掉的进程的选项。roslaunch接收一个或多个XML配置文件，这些文件指定要设置的参数和要启动的节点以及应在其上运行的计算机。

3. rosrun只能运行一个节点，如果要运行多个节点，就需要多次使用rosrun命令，而roslaunch可以采用xml格式描述运行的节点，同时运行多个节点。

**8.请描述视觉SLAM的框架以及各个模块的作用是什么？**

1. 传感器信息读取。在视觉SLAM中主要是相机图像信息的读取和预处理，在机器人中，还会有码盘、惯性传感器等信息的读取和同步。
2. 视觉里程计就是前端，其任务是估算相邻图像间相机运动，以及局部地图的样子。
3. 后端优化。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。
4. 回环检测。判断机器人是否到达过去先前的位置，如果检测到回环，它会把信息提供给后端进行检测。
5. 建图。根据估计的轨迹，建立与任务要求对应的地图。

**9.SLAM中的绑架问题是什么？**

绑架问题就是重定位，指的是机器人缺少先前位置信息的情况下确定当前位姿。比如机器人在一个已经构建好地图的环境中，但它并不知道自己在地图中的相对位置，或者在移动过程中，由于传感器的暂时性功能故障或者相机的快速移动，导致先前的位置信息丢失，因此得重新确定机器人的位置。初始化绑架是一个通常状况的初始化问题，可以使用粒子滤波方法，重新分散例子到三维空间，被里程信息和随机扰动不断更新，初始化粒子收敛到可解释观察结果的区域。追踪丢失状态绑架，即在绑架发生之前，系统已经保存当前状态，则可以使用除视觉传感器之外的其他的传感器作为候补测量设备。

**10.在视觉SLAM中可能用到有关的边缘检测算子有哪些？**

在边缘检测一般分为滤波、增强和检测三个步骤，其基本原理是用高斯滤波器进行去噪，之后再用卷积内核寻找像素梯度。边缘检测算子：

1. canny算子：一种完善的边缘检测算法，抗噪能力强，用高斯滤波平滑图像，用一阶偏导的有限差分计算梯度的幅值和方向，对梯度幅值进行非极大值抑制，采用双阈值检测和连接边缘。

2. sobel算子：一阶导数算子，引入局部平均运算，对噪声具有平滑作用，抗噪声能力强，计算量较大，但定位精度不高，得到的边缘比较粗，适用于精度要求不高的场合。
3. laplacian算子：二阶微分算子，具有旋转不变性，容易受噪声影响，不能检测边缘的方向，一般不直接用于检测边缘，而是判断明暗变化。

**11.在SLAM中，如何对匹配好的点做进一步的处理，更好保证匹配效果？**

1. 确定匹配最大距离，汉明距离小于最小距离的两倍。

2. 使用KNN-matching算法，在这里设置K为2，每个匹配得到两个最接近的描述子，然后计算最接近距离和次接近距离之间的比值，当比值大于既定值时，才作为最终匹配。

3. 使用RANSAC算法找到最佳单应性矩阵，该函数使用的特征点同时包含正确和错误匹配点，因此计算的单应性矩阵依赖于二次投影的准确性。

**12.SLAM后端有滤波方法和非线性优化方法，这两种方法的优缺点是什么？**

滤波方法的优点：在当前计算资源受限、待估计量比较简单的情况下，EKF为代表的滤波方法非常有效，经常用在激光SLAM中。

滤波方法的缺点：存储量和状态量是平方增长关系，因为存储的是协方差矩阵，因此不适合大型场景。但是现在视觉SLAM的方案中特征点的数据很大，滤波方法效率是很低的。

非线性优化方法一般以图优化为代表，在图优化中BA是核心，而包含大量特征点和相机位姿的BA计算量很大，无法实时。在后续的研究中，人们研究了SBA和硬件加速等先进方法，实现了实时的基于图优化的视觉SLAM方法。

**13.什么是BA优化？**

BA的全称是Bundle Adjustment优化，指的是从视觉重建中提炼出最优的三维模型和相机参数，包括内参和外参。从特征点反射出来的几束光线，在调整相机姿态和特征点空间位置后，最后收束到相机光心的过程。BA优化和冲投影的区别在于，对多段相机的位姿和位姿下的路标点的空间坐标进行优化。

将误差表示为：

![img](https://pic2.zhimg.com/80/v2-294cb9ef5ade23aa9ea3b67fc0496c21_720w.jpg)

也就是：

![img](https://pic2.zhimg.com/80/v2-d4c9094d2e278460be9047c89c0f13ed_720w.jpg)

可以计算出误差对位姿和路标坐标的偏导：

![img](https://pic1.zhimg.com/80/v2-e808b1ba49069c2b422e05e6f29cf130_720w.jpg)

![img](https://pic3.zhimg.com/80/v2-855b719a57b7fd914fcb587b516a3b02_720w.jpg)

对于特征点位置p和m个位姿以及n个特征点，表示为：

![img](https://pic4.zhimg.com/80/v2-bdd2a3a4f470be5c565fc02dce33001b_720w.jpg)

上式中的右边简写为：

![img](https://pic4.zhimg.com/80/v2-9d6b95f3bd263d192cd111b644ca002b_720w.jpg)

![img](https://pic2.zhimg.com/80/v2-202c8a00d820221fa425d66704a51a79_720w.jpg)

然后，将优化的目标函数表示为：

![img](https://pic4.zhimg.com/80/v2-a6edd7b11e6a1085058e972ec6b3aca7_720w.jpg)

目标函数也可以表示为：

![img](https://pic2.zhimg.com/80/v2-d73b6ec9a9c4469ff0462cef05ce78d9_720w.jpg)

对于线性增量方程：

![img](https://pic4.zhimg.com/80/v2-72aae0905e8ad18a7fca70c8deda27d7_720w.jpg)

这里的

![img](https://pic1.zhimg.com/80/v2-b739cbad56f8e9959c7d2ff7b4e56cf0_720w.jpg)

将雅克比矩阵分块为：



![img](https://pic4.zhimg.com/80/v2-56adc1eaf1025d3123f0610f075ae1ab_720w.jpg)

则：



![img](https://pic1.zhimg.com/80/v2-223bcba5fb20456cc368ee9c1dc6222c_720w.jpg)

**14.描述一下RANSAC算法。**

RANSAC算法是随机采样一致算法，从一组含有“外点”的数据中正确估计数学模型参数的迭代算法。“外点”一般指的是数据中的噪声，比如匹配中的误匹配和估计曲线中的离群点。因此，RANSAC算法是一种“外点”检测算法，也是一种不确定的算法，只能在一种概率下产生结果，并且这个概率会随着迭代次数的增加而加大。RANSAC主要解决样本中的外点问题，最多可以处理50%的外点情况。

RANSAC主要通过反复选择数据中的一组随机子集来达成目标，被选取的子集假设为局内点，验证步骤如下：

1. 一个模型适用于假设的局内点，也就是说所有的未知参数都能从假设的局内点计算得到。
2. 使用（1）中得到的模型测试所有其他数据，如果某个点适用于估计的模型，认为它也是局内点。
3. 如果有足够多的点被归类为假设的局内点，则估计的模型就足够合理。
4. 使用假设的局内点重新估计模型，因为它仅仅被初始的假设局内点估计。
5. 最终，通过估计局内点和模型的错误率估计模型。

**15.[相似变换、仿射变换、射影变换的区别](https://link.zhihu.com/?target=https%3A//www.freesion.com/article/35571024388/%238.%20%E7%9B%B8%E4%BC%BC%E5%8F%98%E6%8D%A2%E3%80%81%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E3%80%81%E5%B0%84%E5%BD%B1%E5%8F%98%E6%8D%A2%E7%9A%84%E5%8C%BA%E5%88%AB)是什么？**

相似变换相当于等距变换和均匀缩放的一个复合，用S表示变换矩阵，S为3×3矩阵，

![img](https://pic4.zhimg.com/80/v2-2d57b2aaf841bf7d801386ee2a13cd17_720w.jpg)

左上角2×2矩阵为旋转部分，tx和ty为平移因子，具有4个自由度，即旋转、x方向平移、y方向平移和缩放因子s。相似变换前后长度比，夹角，虚圆点I，J保持不变。

仿射变换相当于一个平移变换和一个非均匀变换的复合，用A矩阵表示，A为3×3矩阵，

![img](https://pic1.zhimg.com/80/v2-86b173aac1eb04647fba20cb5dc91678_720w.jpg)

其中A可以分解为：

![img](https://pic3.zhimg.com/80/v2-fc3c67eae2c53eda0e3eec9389f7ad4e_720w.jpg)

其中

![img](https://pic4.zhimg.com/80/v2-9e11ab5ac8090a7109aad377f2e4b377_720w.jpg)

左上角2×2矩阵为旋转部分，tx和ty为平移因子，它有6个自由度，即旋转4个，x方向平移，y方向平移。它能保持平移性，不能保持垂直性，图像中各部分变换前后面积比保持不变，共线线段或者平行线段的长度比保持不变，矢量的线性组合不变。

射影变换由有限次中心射影的面积定义的两条直线间的对应变换称为一维射影变换，由有限次中心射影的面积定义的两个平面之间的对应变换称为二维射影变换。射影变换是最一般的线性变换，有8个自由度，保持重合关系和交比不变，但不会保持平行性。

**16.ICP算法的原理是什么？简要叙述一下。**

ICP算法的核心是最小化目标函数：

![img](https://pic2.zhimg.com/80/v2-29462814168893e4d22087fb0b28240d_720w.jpg)

目标函数是指是对所有对应点之间的欧式距离的平方和。

**17.四元数的相关概念是什么，请解释一下。**

四元数在程序中使用很广泛，但在SLAM中四元数的概念比较难理解。四元数是Hamilton找到的一种扩展复数，四元数具有一个实部和三个虚部：

![img](https://pic4.zhimg.com/80/v2-c403ad1afb13ced6f1bdccd66c925ca3_720w.jpg)

其中i,j,k是四元数的三个虚部，满足下式：

![img](https://pic2.zhimg.com/80/v2-6c5c49f6e64f3bc778e51561fe0ae1d9_720w.jpg)

也可以使用标量和向量来表示四元数：

![img](https://pic4.zhimg.com/80/v2-a321fccf8d267bffa04e5b719adee697_720w.jpg)

在上式中，标量s是四元数的实部，向量v是虚部。

四元数可以表示三维空间中任意一个旋转，与旋转矩阵类似，假设某个旋转是围绕单位向量

![img](https://pic1.zhimg.com/80/v2-0bbbb4b83a60df286eac80a6ef927b44_720w.jpg)

进行了角度为θ的旋转，则该旋转的四元数形式为：

![img](https://pic2.zhimg.com/80/v2-d1b8a6b4123b0507d159a3c47012ab69_720w.jpg)

上式实质上是模长为1的四元数，也就是单位四元数。反之，也可以通过任意长度为1的四元数计算对应旋转轴和夹角：

![img](https://pic4.zhimg.com/80/v2-0e45729a6ecea8ce2b126028835e3993_720w.jpg)

如果某个四元数的长度不为1，可以通过归一化转化为模长为1的四元数。

对四元数的θ加上2π，就可以得到相同旋转，但对应的四元数变为-q。因此，在四元数中，任意的旋转都可以由两个互为相反数的四元数表示。如果θ为0的话，则得到一个没有任何旋转的四元数：

![img](https://pic1.zhimg.com/80/v2-d97d68486d2dbcd75d798ba9c04ea61c_720w.jpg)

**18.激光SLAM中的具体方法有什么？请解释一下每种方法的特点。**

激光雷达分为单线和多线两种，单线雷达一般应用在平面运动场景，多线雷达应用在三维运动场景。

1. 单线雷达构建二维地图的SLAM算法称为2D lidar SLAM，包括Gmapping、hector、karto和cartographer算法，在二维平面内运动，扫描平面与运动平面平行。
   - Gmapping是一种基于粒子滤波的2D激光雷达SLAM，构建二维栅格地图。融合里程计信息，没有回环检测。优点是在小场景中，计算量小，速度较快。 缺点是每个粒子都携带一幅地图，无法应对大场景（内存和计算量巨大）；如果里程不准或标定参数不准，在长回廊等环境中容易把图建歪。
   - hector SLAM是完全基于s[can](https://link.zhihu.com/?target=http%3A//www.elecfans.com/tags/can/)-matching的，使用迭代优化的方法来求匹配的最佳位置，为避免陷入局部极值，也采用多分辨率的地图匹配。 由于完全依赖于scan matching，要求雷达的[测量](https://link.zhihu.com/?target=http%3A//www.hqchip.com/app/851)精度较高、角度范围大，扫描速度较高（或移动速度慢）。噪声多、边角特征点少的场景就很容易失败。 原文所提出方法的特点还在于，加入IMU，使用EKF估计整体的6DoF位姿，并根据roll, pitch角将激光扫描数据投影到XY平面，因而支持激光雷达有一定程度的倾斜，比如手持或机器人运动在不是很平整的地面上。
   - karto是基于scan-matching，回环检测和图优化SLAM算法，采用SPA（Spa[rs](https://link.zhihu.com/?target=http%3A//www.elecfans.com/tags/rs/)e Pose [Ad](https://link.zhihu.com/?target=https%3A//dfm.elecfans.com/uploads/software/hqdfm.zip%3Fneilian)justment）进行优化。
   - cartographer是谷歌开源的激光SLAM框架，主要特点在于： 1.引入submap，scan to submap matching，新到的一帧数据与最近的submap匹配，放到最优位置上。如果不再有新的scan更新到最近的submap，再封存该submap，再去创建新的submap。 2.回环检测和优化。利用submap和当前scan作回环检测，如果当前scan与已经创建的submap在距离上足够近，则进行回环检测。检测到回环之后用ceres进行优化，调整submap之间的相对位姿。为了加快回环检测，采用分枝定界法。

2. 3D lidar SLAM算法是针对多线雷达的SLAM方法，包括LOAM、Lego-LOAM和LOAM-livox等。
   - LOAM是针对多线激光雷达的SLAM算法，主要特点在于：1) 前端抽取平面点和边缘点，然后利用scan-to-scan的匹配来计算帧间位姿，也就形成了里程计；2) 由估计的帧间运动，对scan中的每一个点进行运动补偿；3) 生成map时，利用里程计的信息作为submap-to-map的初始估计，再在利用submap和map之间的匹配做一次优化。 LOAM提出的年代较早（2014），还没有加入回环优化。
   - LeGO-LOAM在LOAM的基础上主要改进：1) 地面点分割，点云聚类去噪；2）添加了ICP回环检测和gtsam优化。
   - LOAM_livox是[大疆](https://link.zhihu.com/?target=http%3A//www.elecfans.com/tags/%E5%A4%A7%E7%96%86/)2019年公布的面向小FOV Lidar的LOAM算法。相比LOAM，做了一些改动。算法的特点： 1.添加策略提取更鲁棒的特征点：a) 忽略视角边缘有畸变的区域; b) 剔除反射强度过大或过小的点 ; c) 剔除射线方向与所在平台夹角过小的点; d) 部分被遮挡的点 2.与LOAM一样，有运动补偿 3.里程计中剔除相对位姿解算后匹配度不高的点（比如运动物体）之后，再优化一次求解相对位姿。

**19.说明UKF，EKF和PF之间的关系。**

从精度的角度来看，所有高精度都是通过增加计算量来换来的，如果UKF通过加权减少Sigma点的方法来降低计算负载，那么精度在一定程度上会低于一阶泰勒展开的EKF线性化。除了EKF和UKF之间的时间复杂性问题外，我们还需要检查它们的理论性能。从以往的一些研究中中，我们知道UKF可以将状态估计和误差协方差预测到4阶精度，而EKF只能预测状态估计的2阶和误差协方差的4阶。但是，只有在状态误差分布中的峰度和高阶矩很明显的情况下，UKF才能进行更准确的估计。在我们的应用中，四元数分量协方差的大小显着小于统一性，这意味着峰度和更高阶矩非常小。这一事实说明了为什么UKF的性能不比EKF好。

另外，采样率也是另外一个拉小UKF与EKF差距的因素，对许多动态模型（有论文提到四元数动态）随着采样间隔的缩短，模型愈发趋近准线性化，那么越小的步长，积分步长把（四元数）传播到单位球面的偏差就越小。因此最小化了线性化误差。

最后，也是最根本的在选取EKF和UKF最直观的因素，UKF不用进行雅可比矩阵计算，但是，许多模型的求导是极为简单的，在最根本的地方UKF没有提供更优的解决方案。这也使得，状态模型的雅可比计算的简单性允许我们在计算EKF和UKF用相同的方法计算过程误差协方差。

UKF并不是万能的，也不是一定比EKF优秀，很多时候需要根据情况选择特定的滤波。

**20.点云配准算法目前有哪些？**

点云配准算法目前有ICP、KC、RPM、形状描述符配准和UPF/UKF。

1. ICP

   ICP算法简单且计算复杂度度低，使它成为最受欢迎的刚性点云配准方法。ICP算法以最近距离标准为基础迭代地分配对应关系，并且获得关于两个点云的刚性变换最小二乘。然后重新决定对应关系并继续迭代知道到达最小值。目前有很多点云配追算法都是基于ICP的改进或者变形，主要改进了点云选择、配准到最小控制策略算法的各个阶段。ICP算法虽然因为简单而被广泛应用。但是它易于陷入局部最大值。ICP算法严重依赖初始配准位置，它要求两个点云的初始位置必须足够近，并且当存在噪声点、外点时可能导致配准失败。

2. KC

   KC算法应用了稳健统计和测量方法。Tsin和Kanade应用核密度估计，将点云表示成概率密度，提出了核心相关（Kernel Correlation，简称KC）算法。这种计算最优配准的方法通过设置两个点云间的相似度测量来减小它们的距离。对全局目标函数执行最优化算法，使目标函数值减小到收敛域。因为一个点云中的点必须和另一个点云中的所有点进行比较，所以这种方法的算法复杂度很高。

3. RPM

   为了克服ICP算法对初始位置的局限性，基于概率论的方法被研究出来。Gold提出了鲁棒点匹配（Robust Point Matching，简称RPM）算法，以及其改进算法。这种方法应用了退货算法减小穷举搜索时间。RPM算法既可以用于刚性配准，也可以用于非刚性配准。对于RPM算法，在存在噪声点或者某些结构缺失时，配准可能失败。

4. 形状描述符配准

   形状描述符配准在初始位置很差的情况下也能大体上很好的实现配准。它配准的前提是假设了一个点云密度，在没有这个特殊假设的情况下，如果将一个系数的点云匹配到一个稠密的点云，这种匹配方法将失败。

5. UPF/UKF

   尽管UPF算法能够精确的配准较小的数据集，但是它需要大量的粒子来实现精确配准。由于存在巨大的计算复杂度，这种方法不能用于大型点云数据的配准。为了解决这个问题，UKF算法被提出来，这种方法收到了状态向量是单峰假设的限制，因此，对于多峰分布的情况，这种方法会配准失败。



> source：[自动驾驶面试题汇总（2022秋招题库）——持续更新](https://www.bilibili.com/read/cv13721554?spm_id_from=333.999.0.0)

**一、惯性导航方向**

IMU测量方程是什么？噪声模型是什么？

惯导误差模型是怎么来的？比如15维的卡尔曼滤波模型。

GPS双天线安装偏角是怎么标定的？

多传感器之间是怎么对时的？

GPS到来时是有延时的，而IMU给出的加速度和角速度是实时的，这种情况下怎么处理延时？怎么做的融合？

DR递推的原理是什么？大概怎么去做？

组合导航卡尔曼滤波过程噪声是如何调参的？

**二、点云算法方向**

最近邻问题有哪几种典型解法？

怎么对KdTree进行插入操作？怎么确定一个节点的分类面？

怎么对KdTree进行Search By Range和Search By KNN操作？

举出除了KdTree以外的近邻计算方法（栅格、B树、R树、四叉、八叉树等）。

给定一组点，如何计算其拟合平面？如何计算其拟合直线？解释其方法的意义。

举出常见的点云的registration方法。

说明ICP的详细过程，要求说明代数解法和匹配问题解法。

说明NDT的详细过程，Normal distribution的含义是什么。

匹配问题除了最近邻还有什么解法？说明匈牙利算法、最大流/最小割、谱方法等其中一种。

解释混合高斯模型含义。解释EM算法的原理。

**三、状态估计方向**

从贝叶斯滤波器角度推出卡尔曼滤波器方程。

从增益最优化角度推出卡尔曼滤波器方程。

从Cholesky方程推出卡尔曼滤波器方程。

解释KF中的噪声矩阵含义。运动方程中估计噪声是变大还是变小？修正方程中估计噪声是变大还是变小？

RTS与KF之间的联系？

将卡尔曼滤波器推广至EKF。

解释数值矩阵求逆的几种做法（QR、Cholesky、SVD）。

什么是Moore-Penrose逆？如何计算Moore-Penrose逆？

SVD是什么？SVD是如何求解的？

特征值是什么？如何计算矩阵的特征值与特征向量？什么样的矩阵必然能对角化？不能对角化的矩阵的标准形式是什么？什么是Jordan标准形？

如何求解线性最小二乘解？如何求解零空间解？说明特征值方法和奇异值方法之间的联系。

描述图优化和最小二乘之间联系。画出VIO中常用的图优化模型。

稀疏图优化的稀疏性体现在哪里？要求答出稀疏Schur和稀疏Cholesky之一。

描述滤波器与最小二乘之间的联系？说明为什么卡尔曼滤波器可以看成两个时刻间的最小二乘。

说明UKF、EKF和PF之间的关系。

解释UKF中的Sigma采样点关系。

解释PF中的重要性重采样的过程是如何做的。解释轮盘赌原理。

解释李群李代数在三维状态估计中的作用。

流形是怎么定义的？流形在局部与R3同胚是什么含义？为什么说SO3是一个流形？

解释SO3, SE3中的Exp和Log的原理。解释BCH的原理和作用。

分别使用左右扰动模型，求解几个常见的雅可比：


22.解释四元数的更新与SO3的更新方式有何异同。

23.说明四元数运动模型与SO3运动模型之间的联系。

24.解释高斯推断和概率学中边缘化之间的关系。解释边缘化与卡尔曼滤波器之间的关系。

25.什么是M估计？说明M估计与核函数之间的关系？

**四、计算机视觉/VIO方向**

单应矩阵、基础矩阵、本质矩阵的定义？

相机内参和外参的含义？如果将图像放大两倍，内外参如何变化？

径向和切向畸变模型含义，鱼眼模型含义（回答等距投影模型即可）？

极线是什么？对极约束是什么？描述了什么几何关系？

八点法原理与过程。

预积分相比于传统积分的差异在哪里？

预积分的测量模型和噪声模型是什么？预积分对零偏是怎么处理的？为什么要这样处理？

说明预积分的图优化模型。

解释重投影模型和BA的原理。

说明PnP问题的解法。

说明RANSAC原理，讨论存在的问题。

解释单目VIO的初始化过程。需要估计哪些量？重力方向和尺度如何估计？

为什么单目VSLAM会有尺度漂移？如何解释这种尺度漂移？现实当中有没有解决办法？

举出几种光流方法（LK，HS等）。说明LK光流的建模方式。

**五.C++方向**

C++函数指针有哪几类？函数指针、lambda、仿函数对象分别是什么？

如何利用谓词对给定容器进行自定义排序？

传递引用和传递值的区别？传递常引用和传递引用之间的区别？传递右值引用和传递引用之间的区别？

函数对象应该通过什么传递？

什么是万能引用？用途是什么？

什么是完美转发？用途是什么？

std::unorded_map和std::map之间的差异是什么？

虚函数、虚表的原理

如何在c++中创建线程？如何在线程间同步？

互斥锁是什么？用途是什么？条件变量又是什么？为什么要用条件变量？

智能指针和祼指针之间的差异？为什么要用指针的引用计数？

智能指针分哪几种？std::unique_ptr, std::shared_ptr, std::weak_ptr各有何用途？

悬挂指针会导致什么问题？如何避免？

traits是什么？什么时候用traits？

参考答案（部分）
**一、惯性导航方向**

1. IMU测量方程是什么？噪声模型是什么？

中值积分的情况下，IMU的测量方程为：


IMU的随机误差一般包括以下几类，各类误差项及其原理如下（以陀螺仪为例）：
（1） 量化噪声
量化噪声是数字传感器必然出现的噪声，我们通过AD采集把连续时间信号采集成离散信号，在这个过程中，精度就会损失，损失的精度大小和AD采样的精度有关（这里具体指的是模数转换时，AD器件的位数，位数越高采样越精确），精度越高，量化噪声越小。
（2） 角度随机游走
陀螺敏感角速率并输出时是有噪声的，这个噪声里面的白噪声成分叫宽带角速率白噪声，我们计算姿态时，本质上是对角速率做积分，这必然会对噪声也做了积分。白噪声的积分并不是白噪声，而是一个马尔可夫过程，即这一次的误差是在上一次误差的基础上累加一个随机白噪声得到的。角度误差所包含的这种马尔可夫性质的误差就叫做角度随机游走。
（3） 角速率随机游走
从理解上和角度随机游走一样，角速率里面并不全是白噪声，它也有马尔可夫性质的误差成分，而这个误差是由宽带角加速率白噪声累积的结果。
（4） 零偏不稳定性噪声
这应该是大家再熟悉不过的一个误差项了，如果一个陀螺只让你用一个指标来体现精度，那必然就是它了。但是这个指标的理解上却不像前几个参数那样直白。
我们可以先把它理解为零偏随时间的缓慢变化，假设在刚开始时零偏大小是某个值，那么过一段时间之后，零偏便发生了变化，具体变化成了多少，无法预估，所以就要给他一个概率区间，来描述它有多大的可能性落在这个区间内，时间越长，区间越大。
实际上，如果你真的测的时间足够长，会发现它也不会无限制增长下去，所以，这个对概率区间的描述只是近似有效，或者一定时间内有效，由于这个有效时间比较长，所以我们一般仍然使用这种方式来描述，只是在理解上要知道这一点的存在。
（5） 速率斜坡
看到斜坡这种描述词，我们一般会想它是不是一种趋势项。实际上，它确实是趋势性误差，而不是随机误差。所谓随机误差，是指你无法用确定性模型去拟合并消除它，最多只能用概率模型去描述它，这样得到的预测结果也是概率性质的。而趋势性误差是可以直接拟合消除的，在陀螺里，这种误差最常见的原因是温度引起零位变化，可以通过温补来消除。
加速度计同样具有这5项误差，而且原理一致，因此不再重复

2. 惯导误差模型是怎么来的？比如15维的卡尔曼滤波模型。
可参考博客文章 < https://zhuanlan.zhihu.com/p/135230133>
或参考英文文献< Quaternion kinematics for the error-state Kalman filter >

3. GPS双天线安装偏角是怎么标定的？
通过车辆前行得到航迹角，同时双天线自己可以计算出一个航向角，两者之差为安装偏角，具体拟合方法可以通过最小二乘或滤波算出。

4. 多传感器之间是怎么对时的？
激光雷达：大多数雷达如VLP-16等都提供基于pps脉冲和GPRMC信号的输入接口,PPS和GPRMC信号可以由GNSS或IMU提供，或者由外部时钟源提供。少数激光雷达还支持NTP/PTP同步，PTP的精度一般来说比NTP要高，这两个信号都需要由外部时钟源设备提供。
相机：需要支持外部触发曝光的型号，因为相机帧周期包括曝光时间和readout时间（整帧像素点读出），一般来说readout时间是固定的，可以补偿这个时间，相机的时间戳选择为曝光的中间时间。
GNSS：GNSS可以从卫星获得高精度的时钟信号，而且通常的GNSS都支持PPS脉冲以及GPRMC信号。
（1）使用GNSS作为时钟源，将GNSS的PPS信号提供给LiDAR和一个开发板，开发板将给相机同时提供一个曝光的脉冲信号。CAMVOX采用这种方案。
（2）使用外部时钟源，这种时钟源通常支持PPS信号输入，将GNSS的PPS传给外部时钟源，同时外部时钟源可以使用PTP/NTP/PPS给LiDAR做时间同步，同时触发相机开始曝光。外部时钟源同时也可以使用PTP/NTP对主机进行时间同步。

5. GPS到来时是有延时的，而IMU给出的加速度和角速度是实时的，这种情况下怎么处理延时？怎么做的融合？
先通过imu积分计算实时的轨迹，同时把imu数据缓存下来，当GPS到来时，再根据GPS的时间戳去修正历史时刻的数据，然后重新积分该时刻后的imu。

6. DR递推的原理是什么？大概怎么去做？
DR，也叫航位推算，是在知道当前时刻位置的条件下，通过测量移动的距离和方位，推算下一时刻位置的方法。可以根据上一时刻位置速度角度，通过imu加速度二次积分得到平移量，角速度积分得到旋转量来进行DR，也可以通过轮速计和车辆运动模型来进行DR。

7. 组合导航卡尔曼滤波过程噪声是如何调参的？
先通过GPS和imu的性能参数和频率确定一个米级单位下的噪声。之后在该噪声参数下得到融合的轨迹，然后分别对两个噪声增大缩小分成几组进行调节，观察轨迹。最终选最平滑的一组轨迹的噪声参数，或者选跟真值比精度最高的一组的噪声参数 

### 2022最新自动驾驶面试题汇总（持续更新中）

> source：[深蓝学院：2022最新自动驾驶面试题汇总（持续更新中）](https://zhuanlan.zhihu.com/p/547586531)

**参考答案**

**1.ROS的通信机制有哪些？**

ROS基本的通信方式有三种：话题通信（发布订阅模式）、服务通信（请求响应模式）和参数服务器（参数共享模式）。

话题通信指的是一个节点发布消息，另一个节点订阅消息，适用于不断更新的、少逻辑处理的数据传输场景。话题通信涉及到Talker（发布者）、Listener（订阅者）和ROS Master（管理者）三个角色，发布者负责给订阅者发布数据，订阅者负责接收发布者的数据，管理者负责保管发布者和订阅者注册的消息，并匹配话题相同的发布者与订阅者，帮助发布者与订阅者建立连接。

服务通信是基于请求响应模式的，是一种应答机制。也即: 一个节点A向另一个节点B发送请求，B接收处理请求并产生响应结果返回给A。一般用于偶然的、对时时性有要求、有一定逻辑处理需求的数据传输场景。服务通信涉及到Client（客户端）、Server（服务端）和ROS Master（管理者）三个角色，客户端负责向服务端发送请求，并接收服务端发送的数据，服务端负责接收处理请求，并对客户端做出相应，管理者负责保管客户端和服务端注册的信息，并匹配话题相同的客户端与服务端，帮助客户端与服务端建立连接。

参数服务器在ROS中主要用于实现不同节点之间的数据共享。参数服务器相当于是独立于所有节点的一个公共容器，可以将数据存储在该容器中，被不同的节点调用，当然不同的节点也可以往其中存储数据。参数服务器一般用于存储一些多节点共享的数据，类似于全局变量。参数服务器涉及ROS Master（管理者）、Talker（参数设置者）和Listener（参数调用者），管理者作为一个公共的容器保存数据，参数设置者往容器中储存数据，参数调用者读取容器中所需的数据。

**2.BFS和DFS的区别是什么？**

BFS是广度优先搜索，其特点是：

（1）BFS从根节点开始搜索，并根据树级别模式探索所有邻居根。

（2）它使用队列数据结构来记住下一个节点访问。

（3）BFS比DFS需要更多的内存。

（4）它是使用FIFO列表应用的。

（5）寻找最短路径的理想选择。

（6）该算法用于查找两个节点之间的最短路径，发现图中的所有连接组件，分析图是否为二部图等。

DFS是深度优先搜索，其特点是：

（1）DFS从根节点开始搜索，并从根节点尽可能远地探索这些节点。

（2）使用堆栈数据结构来记住下一个节点访问。

（3）DFS所需的内存少于BFS所需的内存。

（4）它是通过LIFO列表应用的。

（5）寻找最短距离的理想选择。

（6）该算法用于解决问题，拓扑排序，需要对图进行回溯，识别图中的循环以及发现两个节点之间的路径等。

**3.进程的通信方式是什么？**

进程之间的通信方式一般有管道、FIFO、消息队列、信号量和共享内存五种。

管道指的是无名管道，是UNIX 系统IPC最古老的形式。其特点如下：

（1）它是半双工的（即数据只能在一个方向上流动），具有固定的读端和写端。

（2）它只能用于具有亲缘关系的进程之间的通信（也是父子进程或者兄弟进程之间）。

（3）它可以看成是一种特殊的文件，对于它的读写也可以使用普通的read、write 等函数。但是它不是普通的文件，并不属于其他任何文件系统，并且只存在于内存中。

FIFO是命名管道，是一种文件类型。其特点如下：

（1）FIFO可以在无关的进程之间交换数据，与无名管道不同。

（2）FIFO有路径名与之相关联，它以一种特殊设备文件形式存在于文件系统中。

消息队列是消息的链接表，存放在内核中。一个消息队列由一个标识符（即队列ID）来标识。其特点如下：

（1）消息队列是面向记录的，其中的消息具有特定的格式以及特定的优先级。

（2）消息队列独立于发送与接收进程。进程终止时，消息队列及其内容并不会被删除。

（3）消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取。

信号量是一个计数器，用于实现进程间的互斥和同步，其特点如下：

（1）信号量用于进程间同步，若要在进程间传递数据需要结合共享内存。

（2）信号量基于操作系统的 PV 操作，程序对信号量的操作都是原子操作。

（3）每次对信号量的 PV 操作不仅限于对信号量值加1或减1，而且可以加减任意正整数。

（4）支持信号量组。

共享内存指的是两个或多个进程共享一个给定的存储区，其特点如下：

（1）共享内存是最快的一种 IPC，因为进程是直接对内存进行存取。

（2）因为多个进程可以同时操作，所以需要进行同步。

（3）信号量+共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。

**4.简述BN归一化的过程。**

BN是Batch Normalization的缩写，该方法的代表性论文是“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”，论文中关于BN的解释是训练深度神经网络非常复杂,因为在训练过程中,随着先前各层的参数发生变化,各层输入的分布也会发生变化,图层输入分布的变化带来了一个问题,因为图层需要不断适应新的分布,因此训练变得复杂,随着网络变得更深,网络参数的细微变化也会放大。

由于要求较低的学习率和仔细的参数初始化,这减慢了训练速度,并且众所周知,训练具有饱和非线性的模型非常困难。我们将此现象称为内部协变量偏移,并通过归一化层输入来解决该问题。

![img](https://pic2.zhimg.com/80/v2-5d2a18304155bccefab64bd624d6af39_720w.jpg)

如上图中的左图所示,由于两个数据不在同一范围,但它们是使用相同的学习率,导致梯度下降轨迹沿一维来回振荡,从而需要更多的步骤才能达到最小值。且此时学习率不容易设置,学习率过大则对于范围小的数据来说来回震荡,学习率过小则对范围大的数据来说基本没什么变化。如上图中的右图所示,当进行归一化后,特征都在同一个大小范围,则loss landscape像一个碗,学习率更容易设置,且梯度下降比较平稳。

算法的核心代码如下：

![img](https://pic2.zhimg.com/80/v2-9e2927ab4235a0456ec2896c05e51b4d_720w.jpg)

在一个batch中,在每一BN层中,对每个样本的同一通道,计算它们的均值和方差,再对数据进行归一化,归一化的值具有零均值和单位方差的特点,最后使用两个可学习参数gamma和beta对归一化的数据进行缩放和移位。

此外,在训练过程中还保存了每个mini-batch每一BN层的均值和方差,最后求所有mini-batch均值和方差的期望值,以此来作为推理过程中该BN层的均值和方差。

**5.自动驾驶汽车闯红灯的原因是什么？**

闯红灯的原因在于：

摄像机里的无线收发装置产生了电磁干扰，以致交通信号灯上的转发器所发出的信号受到破坏。

这个事故显然让以往被人忽视的电磁干扰问题更加突显出来。而且，对于很多汽车制造商来说，干扰来自很多目前应用的技术，从车内的手机充电器，到电子控制的转向和刹车系统等等。

仅仅是确定干扰出自何处，就是一个极具挑战性的任务。随着汽车上的电子设备日益增加，计算机化和网联化程度加深，工程师越来越难以辨认干扰的来源。而各种无线技术彼此交叉重叠，无疑让挑战更加复杂。

美国联邦通信委员会在十多年前就作出了规定，事先预留了5.9 GHz频段，可通过专用的短程通信，应用于车辆安全的相关领域。但最近几年，蜂窝式汽车互联技术的鼓吹者，以及一些机器人公司，都希望能够共享这个频段。这就为工程师排除干扰的工作增加了不确定性。

电磁干扰的问题显然在大城市、市中心是尤为突出的。而开发自动驾驶的公司恰恰就是期望在这些地方投放它们最初的一批高阶自动驾驶汽车。

然而目前的情况是，许多测试中心都建在远离市区的地方，因此也是无线电干扰可以控制的地方。

从实验到实用，还需要跨越不少鸿沟。

**6.车道线检测的方法有哪些？**

车道线检测方法分为两种：一种是传统算法，主要基于边缘特征或者是图像分割，通过图像预处理，特征提取等方式与卡尔曼滤波器等算法结合，在识别出车道线后采用后处理的方式形成最终的车道。然而这种传统方法易受到光照变化，行驶车辆，道路破损等干扰，导致效果欠佳。近年来，深度学习方法利用网络模型自动学习目标特征，具有较强的泛化能力，可以有效提高目标检测的准确率。得益于卷积神经网络的强大特征提取能力，性能也在不断提升。

**7.自动驾驶中构建的地图都有哪些形式？**

二维地图：栅格地图、拓扑地图、导航网格图。

栅格地图主要对平面进行[网格](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E7%BD%91%E6%A0%BC%26spm%3D1001.2101.3001.7020)化，格子一般为正方形，长度的调整代表地图的分辨率。

![img](https://pic2.zhimg.com/80/v2-61bac21ec18878497cbe65bf0364e1fd_720w.jpg)

拓扑地图与图论紧密结合，通过抽象的点与边，边的方向与路径长度代表一个地图的信息。

![img](https://pic2.zhimg.com/80/v2-e6074bd92a9b8a663213edbf92cfb1b1_720w.jpg)

导航网格图弥补了栅格地图的缺陷，网格由不同的多边形构成，多边形一般根据障碍物顶点的连线构成，将地图划分为不规则的一个个区域，网格数量较少。

![img](https://pic1.zhimg.com/80/v2-5481e8805b4cb0a91f874737cddbccb8_720w.jpg)

三维地图：栅格地图、八叉树地图、点云地图、体素地图、TSDF地图、ESDF地图。

栅格地图类似于二维栅格地图，三维地图相对比较占内存。

![img](https://pic3.zhimg.com/80/v2-bf7959ef449d3acb13a6b27113b611e2_720w.jpg)

八叉树地图如果某个位置上没有障碍物，那么可以用该位置上的大正方体表示，如果某个位置上是小障碍物，那就把大正方体切分到刚好可以包含这个障碍物的小正方形。这样就可以减少计算量，节约内存。而这刚好是八叉树这种数据结构，寻找障碍物先从最大的正方体开始找，然后再向由这个正方体平均分的八个正方体里去找。

![img](https://pic3.zhimg.com/80/v2-2e6ea0e86d8323930fc85f42b81afad6_720w.jpg)

点云地图通过激光雷达等传感器得到数据在进行三维重建得到点云地图。

![img](https://pic3.zhimg.com/80/v2-869ee8879ca25be97b9c3c3703e36746_720w.jpg)

体素地图类似于八叉树地图，由最小的正方体为单位构成，每个体素存储一个SDF、颜色和权重。

TSDF地图是截断符号距离函数场，这里的体素储存的是投影距离（projective distance），就是沿着传感器射线到已测量的表面的距离。

![img](https://pic4.zhimg.com/80/v2-62bd5bf1b8ab24ae5b806aadbb06ef57_720w.jpg)

ESDF地图是欧几里得符号距离场，在这个场里面的每一个体素都包含了距离它最近的障碍物的欧几里得距离。

![img](https://pic2.zhimg.com/80/v2-832d9fb848c98566ff264e990dccf77d_720w.jpg)

**8.自动驾驶中MPC控制的过程。**

模型预测控制（Model Predictive Control）指一类算法，周期性基于当帧测量信息在线求解一个有限时间开环优化问题，并将结果的前部分控制序列作用于被控对象。根据所用模型不同，分为动态矩阵控制（DMC），模型算法控制（MAC）、广义预测控制（GPC）。在智能驾驶方向，重点在于基于状态空间模型的模型预测控制。

预测控制最大的吸引力在于它具有显式处理约束的能力, 这种能力来自其基于模型对系统未来动态行为的预测, 通过把约束加到未来的输入、输出或状态变量上, 可以把约束显式表示在一个在线求解的二次规划或非线性规划问题中。

在线求解开环优化问题获得开环优化序列是MPC和传统控制方法的主要区别，因为后者通常是离线求解一个反馈控制律，而该反馈控制律一旦确定，在系统中就不再变动。



![img](https://pic4.zhimg.com/80/v2-88433781a5d3449a0fbde5fe133bdbe3_720w.jpg)

其中，x(t)为t时刻车辆的测量状态量，x*(t)为t时刻车辆的估计状态，u’(t)为t时刻的最优控制解，y(t)为t时刻的系统输出。

MPC算法包括三个步骤：

预测模型：根据历史信息、当前输入预测未来输出。我们需要一个模型能够基于历史信息和当前状态，来预测未来输出，这就涉及状态量的描述，非线性模型的线性化，从而确保预测输出最大限度接近期望值。

滚动优化：某一性能指标最优，反复在线优化。由于外部干扰，模型系统误差等原因，预测输出与实际存在偏差，滚动优化要做的就是找到每个时刻下的局部最优解，一般会设计一个损失函数，转化为二次规划问题，找到最优解。

反馈校正：基于测量对模型预测进行修正。模型基于当前与过去信息预测未来输出，那么未来时刻的输出就是反馈信息，这一部分与模型有较大关系，有些模型把这一部分内容体现在前两步骤中。

**9.目前的目标检测方法有哪些？**

目标检测的方法包括传统方法和深度学习方法。

传统目标检测方法有VJ检测器、HOG检测和DPM等。

（1）VJ检测器

采用最直接的检测方法，即，滑动窗口：查看图像中所有可能的位置和比例，看看是否有窗口包含人脸。虽然这似乎是一个非常简单的过程，但它背后的计算远远超出了计算机当时的能力。VJ检测器结合了 “ 积分图像 ”、“ 特征选择 ” 和 “ 检测级联 ” 三种重要技术，大大提高了检测速度。

（2）HOG检测

HOG可以被认为是对当时的尺度不变特征变换（scale-invariant feature transform）和形状上下文（shape contexts）的重要改进。为了平衡特征不变性 ( 包括平移、尺度、光照等 ) 和非线性 ( 区分不同对象类别 )，将HOG描述符设计为在密集的均匀间隔单元网格上计算，并使用重叠局部对比度归一化 ( 在“块”上 ) 来提高精度。虽然HOG可以用来检测各种对象类，但它的主要动机是行人检测问题。若要检测不同大小的对象，则HOG检测器在保持检测窗口大小不变的情况下，多次对输入图像进行重新标度。多年来，HOG检测器一直是许多目标检测器和各种计算机视觉应用的重要基础。

（3）DPM

DPM算法采用了改进后的HOG特征，SVM分类器和滑动窗口（Sliding Windows）检测思想，针对目标的多视角问题，采用了多组件（Component）的策略，针对目标本身的形变问题，采用了基于图结构（Pictorial Structure）的部件模型策略。此外，将样本的所属的模型类别，部件模型的位置等作为潜变量（Latent Variable），采用多示例学习（Multiple-instance Learning）来自动确定。

深度学习目标检测方法有基于分类的检测算法和基于回归的检测算法，基于分类的检测算法有OverFeat算法、R-CNN、SPP-Net、Fast-RCNN、Faster-RCNN、R-FCN、Mask R-CNN，基于回归的检测算法有YOLO、SSD、YOLOv2以及YOLO9000、YOLOv3、YOLOv4、YOLOv5和YOLOvx。

（1）OverFeat算法

OverFeat并没有使用region proposal，但其思路被后面的R-CNN系列沿用并改进。该算法通过多尺度的滑动窗口结合 AlexNet提取图像特征，完成检测。在 ILSVRC 2013 数据集上的平均准确率（mean Average Precision，mAP）为 24.3%，检测效果较传统算法有显著改进，但依旧存在较高错误率。

（2）R-CNN

R-CNN利用Selective Search获得候选区域（约2000个）。随即对候选区域大小进行归一化，用作CNN网络的标准输入。再使用AlexNet获得候选区域中的特征，最后利用多个 SVM 进行分类以及线性回归微调定位框（Bounding-box）。

但是，R-CNN对近2 000个候选区域分别做特征提取，而候选区域之间存在许多重复区域，导致大量且重复的运算，运行缓慢，平均每幅图片的处理时间为 34 s。同时，对每一步的数据进行存储，极为损耗存储空间。另外，对候选区域进行归一化操作，会对最终结果产生影响。

（3）SPP-Net

针对R-CNN对所有候选区域分别提取特征的缺点，SPP-Net一次性对整张图片作卷积操作提取特征。使得特征提取从 R-CNN 的近 2 000 次变为提取1次整张图片特征，大大减少了工作量。

另外，SPP-Net在最后一个卷积层后、全连接层前添加空间金字塔池化层（SPP层），提取固定尺寸的特征向量，避免对候选区域大小进行归一化的复杂操作。

（4）Fast-RCNN

Fast R-CNN算法在SPP-Net的基础上，将SPP层简化为ROI Pooling层，并将全连接层的输出作SVD分解，得到两个输出向量：softmax的分类得分以及 Bounding box外接矩形框的窗口回归。这种改进将分类问题和边框回归问题进行了合并；用 softmax 代替 SVM，将所有的特征都存储在显存中，减少了磁盘空间的占用；SVD分解则在几乎不影响精度的情况了，极大加快检测速度。

Fast R-CNN 使用 VGG16 代替 AlexNet，平均准确率达到 70.0%，且训练速度较 R-CNN 提升 9 倍，检测速度达到每幅图片 0.3 s（除去 region proposal 阶段）。Fast R-CNN 依然使用 Selective Search 方法选取候选区域，这一步骤包含大量计算。

Fast RCNN成功地融合了 R-CNN 和 SPPNet 的优点，但其检测速度仍然受到建议检测候选区域的限制 。由此可见，改进Selective Search是Fast R-CNN速度提升的关键。

（5）Faster-RCNN

Faster R-CNN使用RPN网络（Region Proposal Networks）替代 Selective Search 算法，使目标识别实现真正端到端的计算。

RPN 使得 Faster R-CNN 在 region proposal 阶段只需10 ms，检测速度达到5 f/s（包括所有步骤），并且检测精度也得到提升，达到 73.2%。但是，Faster R-CNN 仍然使用 ROI Pooling，导致之后的网络特征失去平移不变性，影响最终定位准确性；ROI Pooling后每个区域经过多个全连接层，存在较多重复计算；Faster R-CNN 在特征图上使用锚点框对应原图，而锚点框经过多次下采样操作，对应原图一块较大的区域，导致 Faster R-CNN检测小目标的效果并不是很好。

（6）R-FCN

R-FCN使用全卷积网络ResNet代替VGG，提升特征提取与分类的效果；针对全卷积网络不适应平移敏感性的缺陷，该算法使用特定的卷积层生成包含目标空间位置信息的位置敏感分布图（Position Sensitive Score Map）；ROI Pooling 层后不再连接全连接层，避免重复计算。

R-FCN的准确率达到 83.6%，测试每张图片平均花费170 ms，比Faster-RCNN快了2.5~20倍。但是R-FCN在得到 Score map 需要生成一个随类别数线性增长的channel数，这一过程虽然提升了目标检测精度，但减慢了检测速度，导致其难以满足实时性要求。

（7）Mask R-CNN

Mask R-CNN是一种在 Faster R-CNN 基础上加以改进的算法，增加了对实例分割的关注。该算法在分类和定位回归以外，加入了关于实例分割的并行分支，并将三者的损失联合训练。实例分割要求实例定位的精准度达到像素级，而Faster R-CNN 因为 ROI Pooling 层的等比例缩放过程中引入了误差，导致空间量化较为粗糙，无法准确定位。

Mask R-CNN 提出双线性差值 RoIAlign 获得更准确的像素信息，使得掩码（mask）准确率提升 10%到50%；Mask R-CNN 还使用 ResNeXt基础网络，在COCO 数据集上的检测速度为 5 f/s，检测准确性从 Fast-RCNN的19.7%提升至39.8%。

Mask R-CNN在检测精度、实例分割方面都达到目前最高的层次。其后一些算法在性能上有所提升，但基本维持在同一水平。但是该算法的检测速度依旧难以满足实时要求，并且实例分割目前也还面临着标注代价过于昂贵的问题。

（8）YOLO

YOLO v1（2016）将图片划分为 S × S 的网格（cell），各网格只负责检测中心落在该网格的目标，每个网格需要预测两个尺度的bounding box和类别信息，一次性预测所有区域所含目标的bounding box、目标置信度以及类别概率完成检测。

YOLO 采用以 cell为中心的多尺度区域取代 region proposal，舍弃了一些精确度以换取检测速度的大幅提升，检测速度可以达到 45 f/s，足以满足实时要求；检测精度为63.4%，较Faster R-CNN的73.2%，差距较大。

YOLO在极大提高检测速度的情况下，也存在以下问题：（1）因为每个网格值预测两个bounding box，且类别相同，因此对于中心同时落在一个网格总的物体以及小物体的检测效果差，多物体环境下漏检较多；（2）由于YOLO关于定位框的确定略显粗糙，因此其目标位置定位准确度不如 Fast-RCNN；（3）对于外型非常规的物体检测效果不佳。

（9）SSD

Faster-RCNN 检 测 检 测 精 度 高 但 检 测 速 度 慢 ，YOLO 检测精度不高但检测速度快，SSD则结合两者的优点，在 YOLO的基础上借鉴了 RPN的思路，在保证高精度检测的同时，兼顾检测速度。

为不同层的特征图具有对应大小的感受野，特定层的特征图只需要训练对应尺度的对象检测。因此，SSD 结合高层和底层的特征图，使用多尺度区域特征进行回归。

结果：SSD300的mAP能达到73.2%，基本与**Faster R-CNN（VGG16）**持平，而检测速度达到59 f/s，比Faster R-CNN快6.6倍。但是SSD具有以下问题：（1）小目标对应到特征图中很小的区域，无法得到充分训练，因此 SSD 对于小目标的检测效果依然不理想；（2）无候选区域时，区域回归难度较大，容易出现难以收敛等问题；（3）SSD不同层的特征图都作为分类网络的独立输入，导致同一个物体被不同大小的框同时检测，重复运算。

（10）YOLOv2和YOLO9000

YOLOv虽然检测速度快，但是它在物体定位方面不够准确，并且召回率低，因此它的检测精度比较低。YOLOv2（2017）通过在每一个卷积层后添加 batch normalization、多尺度训练，加入 K-mean 维度聚类等方式，使得检测速度和精度的再次提升。该算法能够在76.8%正确率的同时达到 67 f/s的检测速度，78.6%的正确率时达到40 f/s。

同时，YOLOv2还专门训练了一个由19个卷积层和5个池化层组成的Darknet-19网络作为模型的主干网络，用于提取特征并减少模型的计算量。同文还提出了YOLO9000，该算法采用wordTree层次分类，混合检测数据、识别数据集，在分类和检测数据集上同时训练，实现9 418类的检测。

无论是 YOLO 系列还是 SSD 算法，都沿用 R-CNN系列算法先在大数据集上进行分类预训练，再在小数据集上 fine-tune 的方法。但 fine-tune 预训练模型有以下问题：（1）预训练模型，往往无法迁移到如医疗图像等特定数据上；（2）预训练模型结构基本固定，难以修改； （3）预训练样本和最终检测目标有所区别，得到的模型未必是检测目标的最佳模型。

（11）YOLOv3

YOLOv3(2018) 在 YOLOv2的基础上，使用全新设计的 Darknet-53 残差网络并结合特征金字塔网络 ( feature pyramid networks，FPN)( Seferbekov 等，2018) 进行多尺度融合预测，其基本思想是先利用特征提取网络得到一定尺寸的特征图(如 13 × 13)，然后将输入图像分成对应个数(13 × 13)的网格单元，如果真实目标的中心坐标落在某一网格单元，则由该网格单元来预测该目标，因为每个网格单元都会预测固定数量的边界框(采用 YOLOv2 中的 K 均值聚类算法(K-means) 获 得 3 个初始尺寸不同的边界框)，最终选择与真实值的 IOU 最大的边界框来预测该目标。

YOLOv3 的Darknet-53 相对于 YOLOv2 的 Darknet-19 改进了两个方面:1) YOLOv3 中做特征图尺寸变化的池化(pooling)层基本由卷积层来实现，减少了模型的运算量;2)针对 YOLOv2 中直筒型网络结构层数太多所产生的梯度问题引入了 ＲesNet 网络中的残差结构(residual blocks)，ＲesNet 的残差结构训练深层网络的难度较小，因此可以将网络做到 53 层来提升检测精度，这些改变使得 YOLOv3 用 1 /3 的时间达到与 SSD 相当的精度。另外 YOLOv3 采用了FPN架构，在三个不同尺度的特征图上进行检测，提高了网络对小目标的检测效果。

（12）YOLOv4

2020年Bochkovskiy等提出了YOLOv4，该模型选择了CSPDarknet53作为主干网络，同时模型中加入了很多普遍适用的算法。例如加权残差连接、跨阶段部分连接、自对抗训练、跨小批量标准化和DropBlock正则化等。这些调优的手段使得该模型实现了当时最优的实验结果。

（13）YOLOv5和YOLOvx

优点：已经证实可以在多个数据集上快速收敛；模型可定制性强

特点：yolov5有点类似EfficientNet网络的设计规则，（输入、宽度、深度）

**10.描述车牌定位的具体过程。**

车牌定位，就是在车牌图像中找出最符合车牌特征的区域。其主要目的是在经图像预处理后原始灰度图像中寻出车牌的位置，并将包含车牌字符的一块子图像从整个图像中分割出来，供字符识别子系统之用，分割的准确与否直接关系到整个车牌字符识别系统的识别率。

车牌识别系统现阶段比较成熟的车牌定位方法有：基于图像的彩色信息法、基于纹理分析的方法、基于边缘检测的方法、基于数学形态学的方法、基于遗传算法的定位、基于神经网络定位等。

汽车牌照定位：在车牌识别系统中对车牌定位的算法包括三个过程，即颜色识别、形状识别、纹理识别。先通过颜色识别来初步确定车牌的所在区域，再结合车牌的形状特征以及纹理特征精确定位。

车牌识别系统都是基于牌照区域的特征来进行定位的，车辆牌照的主要特征如下：

（1）颜色特征

车牌底色与字符颜色有着相应的组合，颜色对比强烈。如果对彩色图像进行定位，有蓝底白字白框线，黄底黑字黑框线，黑底白字白框线，白底黑f红1字黑框线等几种颜色搭配的车牌。如果对灰度图像进行定位，则有深色底浅色字和浅色底深色字两种组合。

（2）几何特征

在车辆图像中，车牌通常是一个近似矩形或者平行四边形的区域。宽、高比例在一定范围内，标准长宽比值为3 .14，定位出来的车牌区域一般在此比值上下，相差不大。

（3）纹理特征

在车辆牌照内，有7个字符规则排列，形成了特殊的纹理特征。通过车辆牌照的扫描线，灰度值有着较规则跳变。

**11.解释SSD算法目标检测的原理。**

早期的目标检测系统包含了两个不同阶段：目标定位和目标检测，这类系统计算量非常耗时，不适用实际应用。Single Shot Detection模型在网络的前向运算中封装了定位和检测，从而显著提高了运算速度。

多尺度特征映射图（Multiscale Feature Maps）：小编认为这是SSD算法的核心之一，原始图像经过卷积层转换后的数据称为特征映射图（Feature Map），特征映射图包含了原始图像的信息。SSD网络包含了多个卷积层，用多个卷积层后的特征映射图来定位和检测原始图像的物体。

先验框（Priors）：在特征映射图的每个位置预先定义不同大小的矩形框，这些矩形框包含了不同的宽高比，它们用来匹配真实物体的矩形框。

预测矩形框：每个特征映射图的位置包含了不同大小的先验框，然后用预测卷积层对特征映射进行转换，输出每个位置的预测矩形框，预测矩形框包含了框的位置和物体的检测分数。比较预测矩形框和真实物体的矩形框，输出最佳的预测矩形框。

损失函数：我们知道了预测的矩形框和真实物体的矩形框，如何计算两者的损失函数？

损失函数包含了位置损失函数和分类损失函数，由于大部分矩形框只包含了背景，背景的位置不需要定位，因此计算两者的位置损失函数用L1函数即可。我们把背景称为负类，包含了物体的矩形框称为正类，不难理解图像中大部分的矩形框只包含了负类，若用全部的负类和正类来计算损失函数，那么训练出来的模型偏向于给出负类的结果。解决办法是在计算分类损失函数时，我们只选择最难检测的几个负类和全部正类来计算。

非极大值抑制（Non-maximum Suppression）:若两个矩形框都包含了相同的物体，且两个矩形框的重叠度较高，则选择分数较高的矩形框，删除分数较低的矩形框。

SSD网络包含了基础网络，辅助卷积层和预测卷积层：

（1）基础网络：提取低尺度的特征映射图；

（2）辅助卷积层：提取高尺度的特征映射图；

（3）预测卷积层：输出特征映射图的位置信息和分类信息。

**12.描述ResNet的基本结构，其设计思想和特点是什么？**

Residual net(残差网络)：靠前若干层的某一层数据输出直接跳过多层引入到后面数据层的输入部分。

残差神经单元：假定某段神经网络的输入是x,期望输出是H(x),如果我们直接将输入x传到输出作为初始结果，那么我们需要学习的目标就是F(x)=H(x)-x，这就是一个残差神经单元，相当于将学习目标改变了，不再是学习一个完整的输出H(x)，只是输出和输入的差别H(x)-x，即残差。

![img](https://pic2.zhimg.com/80/v2-5f785004608fe4034b0bb37fc6bb1bc1_720w.jpg)

残差网络的作用：

（1）普通的直连的卷积网络和ResNet的最大区别在于，ResNet有很多旁路的支线将输入直接连接到后面的层，使得后面的层可以直接学习残差，这种结构也被称为shortcut或者skip connections。

（2）传统的卷积层或全连接层在传递信息时，或多或少会存在信息的丢失，损耗等问题。ResNet在某种程度上解决了这个问题，通过直接将输入信息绕道传到输出，保护信息的完整性，整个网络只需要学习输入，输出差别的那一部分，简化了学习目标和难度。

BatchNormalization(BN)设计思想：

（1）所有输出保证在0到1之间。

（2）所有输出数据均值接近0，标准差接近1的正态分布。使其落入激活函数的敏感区，避免梯度消失，加快收敛。

（3）加快模型收敛速度，并且具有一定的泛化能力。

（4）可以减少Dropout的使用。(减少过拟合)

残差网络ResNets的特点：

（1）残差网络在模型表征方面并不存在直接的优势，ResNets并不能更好的表征某一方面的特征，但是ResNets允许逐层深入地表征更多的模型。

（2）残差网络使得前馈式/反向传播算法非常顺利进行，在极大程度上，残差网络使得优化较深层模型更为简单

（3）“shortcut”快捷连接添加既不产生额外的参数，也不会增加计算的复杂度。快捷连接简单的执行身份映射，并将它们的输出添加到叠加层的输出。通过反向传播的SGD，整个网络仍然可以被训练成终端到端的形式。

**13.介绍Adaboost的原理。**

Adaboost算法基本原理就是将多个弱[分类器](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E5%88%86%E7%B1%BB%E5%99%A8%26spm%3D1001.2101.3001.7020)（弱分类器一般选用单层决策树）进行合理的结合，使其成为一个强分类器。

Adaboost采用迭代的思想，每次迭代只训练一个弱分类器，训练好的弱分类器将参与下一次迭代的使用。也就是说，在第N次迭代中，一共就有N个弱分类器，其中N-1个是以前训练好的，其各种参数都不再改变，本次训练第N个分类器。其中弱分类器的关系是第N个弱分类器更可能分对前N-1个弱分类器没分对的数据，最终分类输出要看这N个分类器的综合效果。

**14.多传感器之间是怎么对时的？**

激光雷达：大多数雷达如VLP-16等都提供基于pps脉冲和GPRMC信号的输入接口，PPS和GPRMC信号可以由GNSS或IMU提供，或者由外部时钟源提供。少数激光雷达还支持NTP/PTP同步，PTP的精度一般比NTP高，这两个信号都需要由外部时钟源设备提供。。

相机：需要支持外部触发曝光的型号，因为相机帧周期包括曝光时间和readout时间，一般来说readout时间是固定的，可以补偿这个时间，相机的时间戳选择为曝光的中间时间。

GNSS：可以从卫星获得高精度的时钟信号，而且通常的GNSS都支持PPS脉冲以及GPRMC信号。

（1）使用GNSS作为时钟源，将GNSS的PPS信号提供给雷达和一个开发板，开发板将给相机一个曝光的脉冲信号。

（2）使用外部时钟源，这种时钟源通常支持PPS信号输入，将GNSS的PPS传给外部时钟源，同时外部时钟源可以使用PTP/NTP/PPS给雷达做时间同步，同时触发相机开始曝光。外部时钟源同时也可以使用PTP/NTP对主机进行时间同步。

**15.说明IMU预积分理论。**

IMU的使用一般是对其测得的加速度，角速度进行积分，从而推算出机器人的位姿。但是，由于积分关系，IMU的积分所得的位姿飘移会随着积分时间的增大而增大。另一方面，当优化方法优化了历史时刻的位姿之后，之后时刻的IMU积分值需要重新进行积分，计算量大。因此，IMU预积分就被提出，以解决以上问题。IMU预积分简单来说就是描述了lidar帧（或者相机帧）之间（时间间隔比较短，比如lidar帧间间隔通常为100ms），IMU数据的观测量。其只跟上一帧时刻的IMU状态量相关，因此，在进行优化过程中，计算量较小。作为观测量，IMU预积分自然是作为一个约束用于slam位姿优化。

**16.说明RANSAC算法的原理。**

RANSAC(RAndom SAmple Consensus,随机采样一致)算法是从一组含有“外点”(outliers)的数据中正确估计数学模型参数的迭代算法。“外点”一般指的是数据中的噪声，比如说匹配中的误匹配和估计曲线中的离群点。所以，RANSAC也是一种“外点”检测算法。RANSAC算法是一种不确定算法，它只能在一种概率下产生结果，并且这个概率会随着迭代次数的增加而加大。RANSAC主要解决样本中的外点问题，最多可处理50%的外点情况。

RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证：

一个模型适用于假设的局内点，即所有的未知参数都能从假设的局内点计算得出。

用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为它也是局内点。

如果有足够多的点被归类为假设的局内点，那么估计的模型就足够合理。

然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过。

最后，通过估计局内点与模型的错误率来评估模型。

这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被舍弃，要么因为它比现有的模型更好而被选用。

**17.简述A\*路径规划方法的特点和基本原理。**

结合了Dijkstra和启发式算法的优点，以从起点到该点的距离加上该点到终点的估计距离之和作为该点在Queue中的优先级。在下文会详细介绍此算法。
优点：比dijkstra快，因评价函数存在，算法会放弃掉图的边缘节点。
缺点：h函数的选择决定了算法速度。

A*算法是启发式搜索，是一种尽可能基于现有信息的搜索策略，也就是说搜索过程中尽量利用目前已知的诸如迭代步数，以及从初始状态和当前状态到目标状态估计所需的费用等信息。

A*算法可以选择下一个被检查的节点时引入了已知的全局信息，对当前结点距离终点的距离作出估计，作为评价该节点处于最优路线上的可能性的量度，这样可以首先搜索可能性大的节点，从而提高了搜索过程的效率。

A* 算法的基本思想如下：引入当前节点j的估计函数f*,当前节点j的估计函数定义为：

f*(j)= g(j)+h*(j)

其中g(j)是从起点到当前节点j的实际费用的量度，h*(j)是从节点j到终点的最小费用的估计，可以依据实际情况，选择h*(j)的具体形式，h*(j)要满足一个要求：不能高于节点j到终点的实际最小费用。从起始节点点向目的节点搜索时，每次都搜索f*(j)最小的节点，直到发现目的节点。

**18.YOLO-v5算法的改进之处体现在哪里？**

改进之处：

（1）Data Augmentation

YOLOV5通过数据加载器传递每一批训练数据，并同时增强训练数据。数据加载器进行三种数据增强：缩放，色彩空间调整和马赛克增强。据悉YOLO V5的作者Glen Jocher正是Mosaic Augmentation的创造者，故认为YOLO V4性能巨大提升很大程度是马赛克数据增强的功劳，也许你不服，但他在YOLO V4出来后的仅仅两个月便推出YOLO V5，不可否认的是马赛克数据增强确实能有效解决模型训练中最头疼的“小对象问题”，即小对象不如大对象那样准确地被检测到。

（2）Auto Learning Bounding Box Anchors

YOLO-v5锚定框是基于训练数据自动学习的。个人认为算不上是创新点，只是手动改代码改为自动运行。

对于自定义数据集来说，由于目标识别框架往往需要缩放原始图片尺寸，并且数据集中目标对象的大小可能也与COCO数据集不同，因此YOLO V5会重新自动学习锚定框的尺寸。

（3）Backbone

YOLO V5和V4都使用CSPDarknet作为Backbone从输入图像中提取丰富的信息特征。CSPNet解决了其他大型卷积神经网络框架Backbone中网络优化的梯度信息重复问题，具体做法是：将梯度的变化从头到尾地集成到特征图中，减少了模型的参数量和FLOPS数值，既保证了推理速度和准确率，又减小了模型尺寸。

（4）Neck

YOLO V5和V4都使用PANET作为Neck来聚合特征。Neck主要用于生成特征金字塔，增强模型对于不同缩放尺度对象的检测，从而能够识别不同大小和尺度的同一个物体。

在PANET之前，一直使用FPN(特征金字塔)作为对象检测框架的特征聚合层，PANET在借鉴 Mask R-CNN 和 FPN 框架的基础上，加强了信息传播。

PANET基于 Mask R-CNN 和 FPN 框架，同时加强了信息传播。该网络的特征提取器采用了一种新的增强自下向上路径的 FPN 结构，改善了低层特征的传播。第三条通路的每个阶段都将前一阶段的特征映射作为输入，并用3x3卷积层处理它们。输出通过横向连接被添加到自上而下通路的同一阶段特征图中，这些特征图为下一阶段提供信息。同时使用自适应特征池化(Adaptive feature pooling)恢复每个候选区域和所有特征层次之间被破坏的信息路径，聚合每个特征层次上的每个候选区域，避免被任意分配。

（5）Head

模型Head主要用于最终检测部分,它在特征图上应用锚定框，并生成带有类概率、对象得分和包围框的最终输出向量。yolo5在通用检测层，与yolo3、yolo4相同。

最后三个特征图是不同缩放尺度的Head被用来检测不同大小的物体，每个Head一共(80个类 + 1个概率 + 4坐标) * 3锚定框，一共255个channels。

（6）Network Architecture

YOLO-v4和YOLO-v5基本相同的网络架构，都使用CSPDarknet53（跨阶段局部网络）作为Backbone，并且使用了PANET（路径聚合网络）和SPP（空间金字塔池化）作为Neck，而且都使用YOLO V3的Head。YOLO V5 s，m，l，x四种模型的网络结构是一样的。原因是作者通过depth_multiple，width_multiple两个参数分别控制模型的深度以及卷积核的个数。

（7）Activation Function

yolo5的作者使用了 Leaky ReLU 和 Sigmoid 激活函数。yolo5中中间/隐藏层使用了 Leaky ReLU 激活函数，最后的检测层使用了 Sigmoid 形激活函数。而YOLO V4使用Mish激活函数。

（8）Optimization Function

YOLO V5的作者提供了两个优化函数Adam和SGD（默认），并都预设了与之匹配的训练超参数。YOLO V4使用SGD。

YOLO V5的作者建议是，如果需要训练较小的自定义数据集，Adam是更合适的选择，尽管Adam的学习率通常比SGD低。但是如果训练大型数据集，对于YOLOV5来说SGD效果比Adam好。

实际上学术界上对于SGD和Adam哪个更好，一直没有统一的定论，取决于实际项目情况。

（9）Benchmarks

YOLO 系列的损失计算是基于 objectness score, class probability score,和 bounding box regression score.

YOLO V5使用 GIOU Loss作为bounding box的损失。

YOLO V5使用二进制交叉熵和 Logits 损失函数计算类概率和目标得分的损失。同时我们也可以使用fl _ gamma参数来激活Focal loss计算损失函数。

YOLO V4使用 CIOU Loss作为bounding box的损失，与其他提到的方法相比，CIOU带来了更快的收敛和更好的性能。

**19.目标检测中IOU的计算过程使用代码编程实现。**

```cpp
#include<iostream>
#include<algorithm>
#include<stdio.h>
#include <vector>
#include<string>
#include<sstream>
#include<map>
#include<set>
#include<iomanip>
#include <functional> // std::greater
using namespace std;
double calcS(vector<int> num)
{
    return (num[2] - num[0])*(num[3] - num[1]);
}
vector<vector<double>> calcIOU(vector<vector<int>> &nums)
{
    vector<vector<double>> res(nums.size(), vector<double>(nums.size(),0.0));
    for (int i = 0; i < nums.size(); ++i)
    {
        for (int j = i + 1; j < nums.size(); ++j)
        {
            int x1 = max(nums[i][0], nums[j][0]);
            int x2 = min(nums[i][2], nums[j][2]);
            int y1 = max(nums[i][1], nums[j][1]);
            int y2 = min(nums[i][3], nums[j][3]);
            double inter_square = (x2 - x1)*(y2 - y1);
            double union_square = calcS(nums[i]) + calcS(nums[j]) - inter_square;
            res[i][j] = inter_square / union_square;
        }
    }
    return res;
} 
int main()
{
    vector<vector<int>> nums;
    vector<vector<double>> res;
    // 表示坐标位置，(x1,y1,x2,y2)，分别是左上角和右下角的坐标
    int a[3][4] = { { 3,6,9,11 },{ 6,3,8,7 },{ 3,7,10,12 } };
    for (int i = 0; i < 3; ++i)
    {
        vector<int> temp;
        for (int j = 0; j < 4; ++j)
        {
            temp.push_back(a[i][j]);
        }
        nums.push_back(temp);
    }
    res = calcIOU(nums);
    for (int i = 0; i < nums.size(); ++i)
    {
        for (int j = i + 1; j < nums.size(); ++j)
            cout << fixed << setprecision(3) <<res[i][j] << " ";
    }
    cout << endl;
    system("pause");
    return 0;
}
```

**20.简述PnP算法的基本步骤和原理。**

PnP(Perspective-n-Point)是求解3D到2D点的对应方法。它描述了当知道n个3D空间点及其位置，如何估计相机的位姿。如果两张图像中的一张特征点3D位置已知，那么至少需要3个点对(以及至少一个额外验证点验证结果)就可以计算相机的运动。PnP的应用范围很广比如两阶段法的6D姿态估计以及视觉SLAM等等。

特征点的3D位置可以由三角化或者RGB-D相机的深度图确定，当然还有其他方法。

PnP求解方法：

（1）DLT直接线性变换

（2）P3P三对点估计位姿

（3）EPnP(Efficient Pnp)

（4）BA(Bundle Adjustment)光速法平差



## 高翔

> source：[想问一下常见SLAM面试编程题？](https://www.zhihu.com/question/532565032/answer/2483264411)

俺举一些自己平时喜欢问的。我比较喜欢问基础问题，所以难度相对低一些：

1. 写一个求两点的距离函数。要求支持常见的2维、3维至任意有限高维点。
2. pose的时间插值是常见需求。写一个6自由度pose的插值函数。pose和时间戳是自定义的结构体，例如 std::vector<TimedPose> 之类。TimedPose.time_stamp为时间，TimedPose.pose是位姿。
3. 让2的函数支持其他的常见有序容器，如std::list<TimedPose>和std::map<double, TimedPose>。
4. 让2的函数支持自定义取时间戳的方式和取pose的方式，因为其他的[结构体](https://www.zhihu.com/search?q=结构体&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2483264411})中时间戳变量和pose变量名称可能不同，例如TimedPose里的时间戳字段是time_stamp，而TimedPose2里的则是time_。
5. 不用库写一个并行的for循环。输入两个[迭代器](https://www.zhihu.com/search?q=迭代器&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2483264411})和一个lambda，例如for_each(begin, end, [](int i){cout<<i;});
6. 用你熟悉的优化库写一个自定的edge/factor/residual function。
7. 写一个读取程序，从rosbag中读取给定消息并执行某个回调函数。回调函数由用户定义。
8. 在7的基础上，把读到的消息进行转换，写入另一个bag包。
9. 在8的基础上，把某种常见的操作在一个通用函数内实现。例如，某个msg含有ros标准header的，将它的header.timestamp转成输出消息的timestamp字段。
10. 写一个三维点的operator < 函数。
11. 如何对空间栅格进行hash？写一个存储空间栅格的hash容器。
12. 把一个同步的ros消息处理函数改成异步的。
13. 在10的基础上，支持任意的消息类型和异步回调函数。
14. 用最小二乘写一个由3D点拟合平面参数的函数。
15. 写一个计算某个数组均值和方差的函数。
16. 在14的基础上，支持高维的矢量数组。
17. 在15的基础上，还能支持对任意结构体的某个字段计算均值和方差。
